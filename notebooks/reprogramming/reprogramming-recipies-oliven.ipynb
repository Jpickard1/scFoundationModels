{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be52c6f-6647-4d93-b0bf-3e05e87917e1",
   "metadata": {},
   "source": [
    "# Reprogramming Recepies\n",
    "\n",
    "Auth: Nat Oliven, Joshua Pickard\n",
    "\n",
    "Date: August 26, 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999b35ad-3c67-4023-bfcd-0da39d281c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sp\n",
    "import anndata as ad\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cb487f-0a53-4ad6-83b8-7d82d2342273",
   "metadata": {},
   "source": [
    "# Day 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54261b2b-fcda-4e0f-90e3-eef3b4ad1376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_symbol</th>\n",
       "      <th>feature_type</th>\n",
       "      <th>ensemblid</th>\n",
       "      <th>highly_variable</th>\n",
       "      <th>means</th>\n",
       "      <th>dispersions</th>\n",
       "      <th>dispersions_norm</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DDX11L1</th>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>False</td>\n",
       "      <td>6.398244e-05</td>\n",
       "      <td>0.835044</td>\n",
       "      <td>-0.573947</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.005574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WASH7P</th>\n",
       "      <td>WASH7P</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000227232</td>\n",
       "      <td>False</td>\n",
       "      <td>2.274395e-03</td>\n",
       "      <td>2.442280</td>\n",
       "      <td>0.533203</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.031731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIR6859-1</th>\n",
       "      <td>MIR6859-1</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000278267</td>\n",
       "      <td>False</td>\n",
       "      <td>6.175251e-05</td>\n",
       "      <td>1.295335</td>\n",
       "      <td>-0.256874</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.005634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIR1302-2HG</th>\n",
       "      <td>MIR1302-2HG</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000243485</td>\n",
       "      <td>False</td>\n",
       "      <td>1.372886e-04</td>\n",
       "      <td>2.656352</td>\n",
       "      <td>0.680668</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.008041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIR1302-2</th>\n",
       "      <td>MIR1302-2</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000284332</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gene_symbol     feature_type        ensemblid  highly_variable  \\\n",
       "DDX11L1          DDX11L1  Gene Expression  ENSG00000223972            False   \n",
       "WASH7P            WASH7P  Gene Expression  ENSG00000227232            False   \n",
       "MIR6859-1      MIR6859-1  Gene Expression  ENSG00000278267            False   \n",
       "MIR1302-2HG  MIR1302-2HG  Gene Expression  ENSG00000243485            False   \n",
       "MIR1302-2      MIR1302-2  Gene Expression  ENSG00000284332            False   \n",
       "\n",
       "                    means  dispersions  dispersions_norm      mean       std  \n",
       "DDX11L1      6.398244e-05     0.835044         -0.573947  0.000039  0.005574  \n",
       "WASH7P       2.274395e-03     2.442280          0.533203  0.001080  0.031731  \n",
       "MIR6859-1    6.175251e-05     1.295335         -0.256874  0.000033  0.005634  \n",
       "MIR1302-2HG  1.372886e-04     2.656352          0.680668  0.000048  0.008041  \n",
       "MIR1302-2    1.000000e-12          NaN          0.000000  0.000000  1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load reprogramming recipes\n",
    "df = pd.read_csv('data/HumanTFs_v_1.01.csv') # now comes from csv file and not straight from web\n",
    "df.head()\n",
    "\n",
    "# Load firboblast source cells\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/unperturbed\"\n",
    "FILE = \"fibroblast.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "\n",
    "adata.var['ensemblid'] = adata.var['ensemblid'].str.split('.').str[0] # JP Add this line\n",
    "\n",
    "adata.var.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a682730-805b-4088-b0ac-c9c87f93778d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: adata.X seems to be already log-transformed.\n",
      "Now clustering kmeans_2_1:\n",
      "kmeans_2_1 has been written to obs\n",
      "Now clustering kmeans_2_2:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Perform K-means clustering\u001b[39;00m\n\u001b[1;32m     34\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mn_clusters, init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk-means++\u001b[39m\u001b[38;5;124m'\u001b[39m, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m \u001b[38;5;241m+\u001b[39m run)\n\u001b[0;32m---> 35\u001b[0m adata\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkmeans_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mfit_predict(X_scaled)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Print that the results have been written to adata.obs\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkmeans_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has been written to obs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1033\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \n\u001b[1;32m   1013\u001b[0m \u001b[38;5;124;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1417\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \n\u001b[1;32m   1392\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m-> 1417\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1418\u001b[0m     X,\n\u001b[1;32m   1419\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1420\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32],\n\u001b[1;32m   1421\u001b[0m     order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1422\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_x,\n\u001b[1;32m   1423\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1424\u001b[0m )\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params_vs_input(X)\n\u001b[1;32m   1428\u001b[0m random_state \u001b[38;5;241m=\u001b[39m check_random_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/utils/validation.py:950\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmay_share_memory(array, array_orig):\n\u001b[0;32m--> 950\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(\n\u001b[1;32m    951\u001b[0m             array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[1;32m    952\u001b[0m         )\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;66;03m# always make a copy for non-numpy arrays\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     array \u001b[38;5;241m=\u001b[39m _asarray_with_order(\n\u001b[1;32m    956\u001b[0m         array, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, xp\u001b[38;5;241m=\u001b[39mxp\n\u001b[1;32m    957\u001b[0m     )\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/utils/_array_api.py:186\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/sklearn/utils/_array_api.py:73\u001b[0m, in \u001b[0;36m_NumPyApiWrapper.asarray\u001b[0;34m(self, x, dtype, device, copy)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21masarray\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Support copy in NumPy namespace\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 73\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39marray(x, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39masarray(x, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import scanpy as sc\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.cluster import KMeans\n",
    "# import numpy as np\n",
    "\n",
    "# # Normalize and log-transform the data\n",
    "# sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "# sc.pp.log1p(adata)\n",
    "\n",
    "# # Extract the count matrix and ensure it is a dense NumPy array\n",
    "# X = adata.X\n",
    "\n",
    "# # Convert sparse matrix to dense if necessary\n",
    "# if hasattr(X, 'toarray'):\n",
    "#     X = X.toarray()\n",
    "\n",
    "# # Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # List of cluster numbers to test\n",
    "# cluster_numbers = list(range(2, 101))\n",
    "\n",
    "# # Number of runs per cluster number\n",
    "# n_runs = 3\n",
    "\n",
    "# # Loop over cluster numbers and runs\n",
    "# for n_clusters in cluster_numbers:\n",
    "#     for run in range(1, n_runs + 1):\n",
    "#         # Print the current iteration\n",
    "#         print(f\"Now clustering kmeans_{n_clusters}_{run}:\")\n",
    "        \n",
    "#         # Perform K-means clustering\n",
    "#         kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=1, random_state=42 + run)\n",
    "#         adata.obs[f'kmeans_{n_clusters}_{run}'] = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "#         # Print that the results have been written to adata.obs\n",
    "#         print(f\"kmeans_{n_clusters}_{run} has been written to obs\")\n",
    "\n",
    "# # Check the updated obs\n",
    "# print(adata.obs.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ed702-ac8e-4d35-95e3-72e853386d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now clustering kmeans_5_1:\n",
      "Silhouette score for kmeans_5_1: -0.0292\n",
      "kmeans_5_1 has been written to obs\n",
      "Now clustering kmeans_5_2:\n",
      "Silhouette score for kmeans_5_2: -0.0914\n",
      "kmeans_5_2 has been written to obs\n",
      "Now clustering kmeans_5_3:\n",
      "Silhouette score for kmeans_5_3: 0.0919\n",
      "kmeans_5_3 has been written to obs\n",
      "Now clustering kmeans_6_1:\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import numpy as np\n",
    "\n",
    "# Normalize and log-transform the data\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "# Extract the count matrix and ensure it is a dense NumPy array\n",
    "X = adata.X\n",
    "\n",
    "# Convert sparse matrix to dense if necessary\n",
    "if hasattr(X, 'toarray'):\n",
    "    X = X.toarray()\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# List of cluster numbers to test\n",
    "cluster_numbers = list(range(5, 50))\n",
    "\n",
    "# Number of runs per cluster number\n",
    "n_runs = 3\n",
    "\n",
    "# Initialize dictionary to store silhouette scores\n",
    "silhouette_scores = {n: [] for n in cluster_numbers}\n",
    "\n",
    "# Loop over cluster numbers and runs\n",
    "for n_clusters in cluster_numbers:\n",
    "    for run in range(1, n_runs + 1):\n",
    "        # Print the current iteration\n",
    "        print(f\"Now clustering kmeans_{n_clusters}_{run}:\")\n",
    "        \n",
    "        # Perform K-means clustering\n",
    "        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=1, random_state=42 + run)\n",
    "        cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "        adata.obs[f'kmeans_{n_clusters}_{run}'] = cluster_labels\n",
    "        \n",
    "        # Calculate silhouette score\n",
    "        if n_clusters > 1:  # Silhouette score is only defined for n_clusters > 1\n",
    "            score = silhouette_score(X_scaled, cluster_labels)\n",
    "            silhouette_scores[n_clusters].append(score)\n",
    "            print(f\"Silhouette score for kmeans_{n_clusters}_{run}: {score:.4f}\")\n",
    "        else:\n",
    "            silhouette_scores[n_clusters].append(None)\n",
    "            print(f\"Silhouette score for kmeans_{n_clusters}_{run}: Not applicable (n_clusters <= 1)\")\n",
    "\n",
    "        # Print that the results have been written to adata.obs\n",
    "        print(f\"kmeans_{n_clusters}_{run} has been written to obs\")\n",
    "\n",
    "# Print the silhouette scores dictionary\n",
    "print(\"Silhouette scores dictionary:\")\n",
    "print(silhouette_scores)\n",
    "\n",
    "# Check the updated obs\n",
    "print(adata.obs.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77faea71-7986-47f3-ad8e-9d58d7681697",
   "metadata": {},
   "source": [
    "# Day 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb14c7-d3e8-45e6-b7fd-d03e8bacf0ca",
   "metadata": {},
   "source": [
    "## Summarize the fibroblast data in adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ffca9-b31b-4699-8791-86b67a1cfb47",
   "metadata": {},
   "source": [
    "### General informtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11bf576-b2d9-4203-874a-4f94e13880e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_symbol</th>\n",
       "      <th>feature_type</th>\n",
       "      <th>ensemblid</th>\n",
       "      <th>highly_variable</th>\n",
       "      <th>means</th>\n",
       "      <th>dispersions</th>\n",
       "      <th>dispersions_norm</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DDX11L1</th>\n",
       "      <td>DDX11L1</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000223972</td>\n",
       "      <td>False</td>\n",
       "      <td>6.398244e-05</td>\n",
       "      <td>0.835044</td>\n",
       "      <td>-0.573947</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.005574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WASH7P</th>\n",
       "      <td>WASH7P</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000227232</td>\n",
       "      <td>False</td>\n",
       "      <td>2.274395e-03</td>\n",
       "      <td>2.442280</td>\n",
       "      <td>0.533203</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.031731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIR6859-1</th>\n",
       "      <td>MIR6859-1</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000278267</td>\n",
       "      <td>False</td>\n",
       "      <td>6.175251e-05</td>\n",
       "      <td>1.295335</td>\n",
       "      <td>-0.256874</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.005634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIR1302-2HG</th>\n",
       "      <td>MIR1302-2HG</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000243485</td>\n",
       "      <td>False</td>\n",
       "      <td>1.372886e-04</td>\n",
       "      <td>2.656352</td>\n",
       "      <td>0.680668</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.008041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MIR1302-2</th>\n",
       "      <td>MIR1302-2</td>\n",
       "      <td>Gene Expression</td>\n",
       "      <td>ENSG00000284332</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gene_symbol     feature_type        ensemblid  highly_variable  \\\n",
       "DDX11L1          DDX11L1  Gene Expression  ENSG00000223972            False   \n",
       "WASH7P            WASH7P  Gene Expression  ENSG00000227232            False   \n",
       "MIR6859-1      MIR6859-1  Gene Expression  ENSG00000278267            False   \n",
       "MIR1302-2HG  MIR1302-2HG  Gene Expression  ENSG00000243485            False   \n",
       "MIR1302-2      MIR1302-2  Gene Expression  ENSG00000284332            False   \n",
       "\n",
       "                    means  dispersions  dispersions_norm      mean       std  \n",
       "DDX11L1      6.398244e-05     0.835044         -0.573947  0.000039  0.005574  \n",
       "WASH7P       2.274395e-03     2.442280          0.533203  0.001080  0.031731  \n",
       "MIR6859-1    6.175251e-05     1.295335         -0.256874  0.000033  0.005634  \n",
       "MIR1302-2HG  1.372886e-04     2.656352          0.680668  0.000048  0.008041  \n",
       "MIR1302-2    1.000000e-12          NaN          0.000000  0.000000  1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load reprogramming recipes\n",
    "df = pd.read_csv('data/HumanTFs_v_1.01.csv') # now comes from csv file and not straight from web\n",
    "df.head()\n",
    "\n",
    "# Load firboblast source cells\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/unperturbed\"\n",
    "FILE = \"fibroblast.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "\n",
    "adata.var['ensemblid'] = adata.var['ensemblid'].str.split('.').str[0] # JP Add this line\n",
    "\n",
    "adata.var.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4764818-4f04-41c9-ba0a-4b45e5f72668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TSP6', 'TSP7', 'TSP4', 'TSP5', 'TSP10', ..., 'TSP3', 'TSP14', 'TSP15', 'TSP1', 'TSP2']\n",
       "Length: 13\n",
       "Categories (13, object): ['TSP1', 'TSP2', 'TSP3', 'TSP4', ..., 'TSP10', 'TSP12', 'TSP14', 'TSP15']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.obs['donor'].values.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c0bb37-5436-406a-9a2f-b1eb80a66e07",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/pandas/core/indexes/base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m adata\u001b[38;5;241m.\u001b[39mobs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/sw/pkgs/arc/python3.11-anaconda/2024.02-1/lib/python3.11/site-packages/pandas/core/indexes/base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3796\u001b[0m     ):\n\u001b[1;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "adata.obs['']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5d7c9e-d6d9-4fb3-b034-8936db373371",
   "metadata": {},
   "source": [
    "### Which are TFs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae91a100-b638-41ff-b61f-9be1a70c89e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a885331-00d0-4e0b-85dc-ee9f9ad810f2",
   "metadata": {},
   "source": [
    "# Day 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac4679-a2a6-4589-a790-c7e91f8173ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Copied from Joshua's notebook, which was copied from mine. Theoretically, this should be Day 10 work with minor changes / debugging,\n",
    "and this is the code the SLURM job is based on.\n",
    "IMPORTANT QUESTION (review code): When we translate gene names from one type to another, are their names actually being updated in either the Human TFs csv or in the adata object?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a2cc3-d1e8-4ca4-a74d-ab1987e2baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reprogramming recipes\n",
    "df = pd.read_csv('data/HumanTFs_v_1.01.csv') # now comes from csv file and not straight from web\n",
    "df.head()\n",
    "\n",
    "# Load firboblast source cells\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/unperturbed\"\n",
    "FILE = \"fibroblast.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "\n",
    "adata.var['ensemblid'] = adata.var['ensemblid'].str.split('.').str[0] # JP Add this line\n",
    "\n",
    "adata.var.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c1246-cedc-4551-83e1-ec9212e3fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate_ids(list):\n",
    "    # Extract base IDs from list_b by removing version numbers\n",
    "    new_list = [id.split('.')[0] for id in list]\n",
    "    return new_list\n",
    "\n",
    "#ensembl ids from the counts matrix\n",
    "counts_ensid_list = adata.var['ensemblid'].values.tolist()\n",
    "\n",
    "# there were version numbers ending with \" .'#' \" that needed to be removed\n",
    "counts_ensid_list = translate_ids(counts_ensid_list)\n",
    "\n",
    "\n",
    "# ensembl ids from the transcription factor list, which in this case is also the perturbation list (testing one at a time)\n",
    "tf_ensid_list = df['Ensembl ID'].values.tolist()\n",
    "\n",
    "\n",
    "def validate_ensembl_tfs(tf_ensid_list, counts_ensid_list):\n",
    "    missing_tfs = []\n",
    "    for TF in tf_ensid_list:\n",
    "        if TF not in counts_ensid_list:\n",
    "            missing_tfs.append(TF)\n",
    "    \n",
    "    if missing_tfs:\n",
    "        print(\"TFs not found in counts_ensid_list:\")\n",
    "        for tf in missing_tfs:\n",
    "            print(tf)\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "validate_ensembl_tfs(tf_ensid_list, counts_ensid_list)\n",
    "\n",
    "# when we perturb each tf in tf_ensid_list, we are just going to skip the one problematic one\n",
    "problem_ens_ids = ['ZNF73_HUMAN', 'ENSG00000204828', 'DUX1_HUMAN', 'DUX3_HUMAN', 'ENSG00000262156', 'ENSG00000196101']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caab2ee5-0c63-40f6-b214-2b6d605713a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_ensid_list))\n",
    "print(len(problem_ens_ids))\n",
    "tf_ensid_list = [id for id in tf_ensid_list if id not in problem_ens_ids]\n",
    "print(len(tf_ensid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1bb523-c283-47f5-a756-59958fcb60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_perturb_id_counts(adata, tf_list, scalar_list):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_id_counts(adata_temp, tf_list, scalar) # JP change this line\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n",
    "def perturb_id_counts(adata, tf_list, scalar): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['ensemblid'].isin(tf_list)\n",
    "    \n",
    "    # Save the original state of the parameter objects, in case some tfs do not translate (failsafe)\n",
    "    original_X = adata.X.copy()\n",
    "    original_gene_mask = gene_mask.copy()\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    max_exp = np.max(adata.X, axis=1)\n",
    "\n",
    "    \"\"\"This is new today. v \"\"\"\n",
    "    # Raise an error if any of the gene names in tf_list do not match column names (we will manually update these in adata):\n",
    "    missing_genes = [gene for gene in tf_list if gene not in adata.var['ensemblid'].values]\n",
    "    \n",
    "    if missing_genes:\n",
    "        # Restore original parameter objects\n",
    "        adata.X = original_X\n",
    "        gene_mask = original_gene_mask\n",
    "        raise ValueError(f\"Genes {missing_genes} not found in anndata object\")\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        # Apply the scaling operation to the specified genes\n",
    "        adata.X[:, gene_mask] = max_exp * scalar\n",
    "        \n",
    "        # Add/Update 'scaled' column in var\n",
    "        adata.var['scaled'] = gene_mask\n",
    "        \n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.var['scaled_by'] = scalar  # Default value for genes not in tf_list\n",
    "\n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.obs['U'] = scalar  # Default value for genes not in tf_list\n",
    "    # adata.var = \n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5abc5-2d5c-49b6-93fc-8f0dfc8aa9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os # JP change this line (all imports go at the top !)\n",
    "\n",
    "output_directory = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/all-tfs\"\n",
    "scalars = [0.5, 0.75, 1.001]\n",
    "\n",
    "# Filter tf_ensid_list to remove problematic Ensembl IDs\n",
    "tf_ensid_list = [id for id in tf_ensid_list if id not in problem_ens_ids]\n",
    "print(f\"Total Ensembl IDs to process: {len(tf_ensid_list)}\")\n",
    "\n",
    "# Clean Ensembl IDs in adata.var['ensemblid'] to remove anything after '.'\n",
    "cleaned_ensembl_ids = adata.var['ensemblid'].str.split('.').str[0]\n",
    "\n",
    "for i in range(len(tf_ensid_list)):\n",
    "    try:\n",
    "        print(f\"\\nProcessing index: {i}\")\n",
    "        val = df['Ensembl ID'].iloc[i]\n",
    "        TFs = val.split()  # Split in case there are multiple TFs\n",
    "        print(f\"Current TFs: {TFs}\")\n",
    "\n",
    "        # Check if all TFs are in the cleaned Ensembl IDs\n",
    "        missing_in_adata = [tf for tf in TFs if tf not in cleaned_ensembl_ids.values]\n",
    "        if missing_in_adata:\n",
    "            print(f\"Skipping missing genes: {missing_in_adata}\")\n",
    "            continue  # Skip this set of TFs if any are missing\n",
    "\n",
    "        # Validate TFs with counts_ensid_list\n",
    "        if validate_ensembl_tfs(tf_ensid_list, counts_ensid_list):\n",
    "            TFs_str = \"_\".join(TFs)\n",
    "            output_path = os.path.join(output_directory, f\"{TFs_str}.h5ad\")\n",
    "\n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"{output_path} already exists: continuing to next TF!\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Perturbing TFs: {TFs}\")\n",
    "            adataDict = iterate_perturb_id_counts(adata.copy(), TFs, scalars)\n",
    "\n",
    "            # Concatenate all AnnData objects along the observations axis\n",
    "            concatenated_adata = ad.concat(list(adataDict.values()), axis=0)\n",
    "\n",
    "            # Copy var information from the original AnnData\n",
    "            concatenated_adata.var = adata.var.copy()\n",
    "\n",
    "            # JP remove the below lines. They were specific to reprogramming information but not important for the human TF\n",
    "            # Save reprogramming metadata into the concatenated_adata.obs table\n",
    "            # concatenated_adata.obs['Source_cells'] = df['Source cells'].iloc[i]\n",
    "            # concatenated_adata.obs['Target_cells'] = df['Target cells'].iloc[i]\n",
    "            # concatenated_adata.obs['Treatment'] = df['Treatment'].iloc[i]\n",
    "            # concatenated_adata.obs['Species'] = df['Species'].iloc[i]\n",
    "            # concatenated_adata.obs['Cell_Transplantation'] = df['Cell Transplantation'].iloc[i]\n",
    "            # concatenated_adata.obs['Published_Year'] = df['Published Year'].iloc[i]\n",
    "            # concatenated_adata.obs['PMID'] = df['PMID'].iloc[i]\n",
    "            # concatenated_adata.obs\n",
    "            \n",
    "            # Save the concatenated AnnData object to the file\n",
    "            concatenated_adata.write_h5ad(output_path)\n",
    "            print(\"    File created successfully\")\n",
    "        else:\n",
    "            print(f\"The file made from {val} could not be created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered at index {i}: {e}\")\n",
    "\n",
    "print('All recipes complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32895a8-0682-4350-96a3-4c83d9e1b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f127a7-839c-4232-8507-3f78fa41bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d943dc-4396-42fe-b736-103d5d647760",
   "metadata": {},
   "source": [
    "# Day 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d209edff-4229-4a0c-a833-0638eed7342c",
   "metadata": {},
   "source": [
    "## Load Data (prev day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef1294c-24df-4d55-b991-68234685928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reprogramming recipes\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Jpickard1/scFoundationModels/main/notebooks/reprogramming/data/HumanTFs_v_1.01.csv')\n",
    "df.head()\n",
    "\n",
    "# Load firboblast source cells\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/unperturbed\"\n",
    "FILE = \"fibroblast.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "\n",
    "\n",
    "adata.var.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764a4f8-37bf-4aed-a35a-2af9c32329f0",
   "metadata": {},
   "source": [
    "## Check for invalid ensembl ids (day 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d8ef0-0eb4-4523-955e-b2798e5e293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def translate_ids(list):\n",
    "    # Extract base IDs from list_b by removing version numbers\n",
    "    new_list = [id.split('.')[0] for id in list]\n",
    "    return new_list\n",
    "\n",
    "#ensembl ids from the counts matrix\n",
    "counts_ensid_list = adata.var['ensemblid'].values.tolist()\n",
    "# there were version numbers ending with \" .'#' \" that needed to be removed\n",
    "counts_ensid_list = translate_ids(counts_ensid_list)\n",
    "\n",
    "\n",
    "# ensembl ids from the transcription factor list, which in this case is also the perturbation list (testing one at a time)\n",
    "tf_ensid_list = df['Ensembl ID'].values.tolist()\n",
    "\n",
    "# def validate_ensembl_tfs(tf_ensid_list, counts_ensid_list):\n",
    "#     for TF in tf_ensid_list:\n",
    "#         if TF not in counts_ensid_list:\n",
    "#             print(TF)\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "\n",
    "def validate_ensembl_tfs(tf_ensid_list, counts_ensid_list):\n",
    "    missing_tfs = []\n",
    "    for TF in tf_ensid_list:\n",
    "        if TF not in counts_ensid_list:\n",
    "            missing_tfs.append(TF)\n",
    "    \n",
    "    if missing_tfs:\n",
    "        print(\"TFs not found in counts_ensid_list:\")\n",
    "        for tf in missing_tfs:\n",
    "            print(tf)\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "validate_ensembl_tfs(tf_ensid_list, counts_ensid_list)\n",
    "\n",
    "# when we perturb each tf in tf_ensid_list, we are just going to skip the one problematic one\n",
    "problem_ens_ids = ['ZNF73_HUMAN', 'ENSG00000204828', 'DUX1_HUMAN', 'DUX3_HUMAN', 'ENSG00000262156', 'ENSG00000196101']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec21c30-6c8b-40a8-82ac-44d7d4842030",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tf_ensid_list))\n",
    "print(len(problem_ens_ids))\n",
    "tf_ensid_list = [id for id in tf_ensid_list if id not in problem_ens_ids]\n",
    "print(len(tf_ensid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a640db-48d1-409e-a836-763d9cc93eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var.head()  # Display the first 10 gene names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef0f84b-55a5-4f1f-8a83-55d0ae5288bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few cleaned Ensembl IDs to verify the format\n",
    "print(cleaned_ensembl_ids[:10])\n",
    "\n",
    "# Check if the specific missing gene is in the cleaned list\n",
    "print('ENSG00000137203' in cleaned_ensembl_ids.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191db3b-890b-493a-913f-e8b6cd6ac484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_perturb_id_counts(adata, tf_list, scalar_list):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_id_counts(adata_temp, tf_list, scalar)\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n",
    "def perturb_id_counts(adata, tf_list, scalar): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['ensemblid'].isin(tf_list)\n",
    "    \n",
    "    # Save the original state of the parameter objects, in case some tfs do not translate (failsafe)\n",
    "    original_X = adata.X.copy()\n",
    "    original_gene_mask = gene_mask.copy()\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    max_exp = np.max(adata.X, axis=1)\n",
    "\n",
    "    \"\"\"This is new today. v \"\"\"\n",
    "    # Raise an error if any of the gene names in tf_list do not match column names (we will manually update these in adata):\n",
    "    missing_genes = [gene for gene in tf_list if gene not in adata.var['ensemblid'].values]\n",
    "    \n",
    "    if missing_genes:\n",
    "        # Restore original parameter objects\n",
    "        adata.X = original_X\n",
    "        gene_mask = original_gene_mask\n",
    "        raise ValueError(f\"Genes {missing_genes} not found in anndata object\")\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        # Apply the scaling operation to the specified genes\n",
    "        adata.X[:, gene_mask] = max_exp * scalar\n",
    "        \n",
    "        # Add/Update 'scaled' column in var\n",
    "        adata.var['scaled'] = gene_mask\n",
    "        \n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.var['scaled_by'] = scalar  # Default value for genes not in tf_list\n",
    "\n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.obs['U'] = scalar  # Default value for genes not in tf_list\n",
    "    # adata.var = \n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537631a6-0137-40c7-afbd-cefdf0d23a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_directory = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/all-tfs\"\n",
    "scalars = [0.5, 0.75, 1.001, 1.25, 1.5]\n",
    "\n",
    "# Filter tf_ensid_list to remove problematic Ensembl IDs\n",
    "tf_ensid_list = [id for id in tf_ensid_list if id not in problem_ens_ids]\n",
    "print(f\"Total Ensembl IDs to process: {len(tf_ensid_list)}\")\n",
    "\n",
    "# Clean Ensembl IDs in adata.var['ensemblid'] to remove anything after '.'\n",
    "cleaned_ensembl_ids = adata.var['ensemblid'].str.split('.').str[0]\n",
    "\n",
    "for i in range(len(tf_ensid_list)):\n",
    "    try:\n",
    "        print(f\"\\nProcessing index: {i}\")\n",
    "        val = df['Ensembl ID'].iloc[i]\n",
    "        TFs = val.split()  # Split in case there are multiple TFs\n",
    "        print(f\"Current TFs: {TFs}\")\n",
    "\n",
    "        # Check if all TFs are in the cleaned Ensembl IDs\n",
    "        missing_in_adata = [tf for tf in TFs if tf not in cleaned_ensembl_ids.values]\n",
    "        if missing_in_adata:\n",
    "            print(f\"Skipping missing genes: {missing_in_adata}\")\n",
    "            continue  # Skip this set of TFs if any are missing\n",
    "        \n",
    "        # Validate TFs with counts_ensid_list\n",
    "        if validate_ensembl_tfs(tf_ensid_list, counts_ensid_list):\n",
    "            TFs_str = \"_\".join(TFs)\n",
    "            output_path = os.path.join(output_directory, f\"{TFs_str}.h5ad\")\n",
    "\n",
    "            # Check if the file already exists\n",
    "            if os.path.exists(output_path):\n",
    "                print(f\"{output_path} already exists: continuing to next TF!\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Perturbing TFs: {TFs}\")\n",
    "            adataDict = iterate_perturb_id_counts(adata.copy(), TFs, scalars)\n",
    "\n",
    "            # Concatenate all AnnData objects along the observations axis\n",
    "            concatenated_adata = ad.concat(list(adataDict.values()), axis=0)\n",
    "\n",
    "            # Copy var information from the original AnnData\n",
    "            concatenated_adata.var = adata.var.copy()\n",
    "\n",
    "            # Save reprogramming metadata into the concatenated_adata.obs table\n",
    "            concatenated_adata.obs['Source_cells'] = df['Source cells'].iloc[i]\n",
    "            concatenated_adata.obs['Target_cells'] = df['Target cells'].iloc[i]\n",
    "            concatenated_adata.obs['Treatment'] = df['Treatment'].iloc[i]\n",
    "            concatenated_adata.obs['Species'] = df['Species'].iloc[i]\n",
    "            concatenated_adata.obs['Cell_Transplantation'] = df['Cell Transplantation'].iloc[i]\n",
    "            concatenated_adata.obs['Published_Year'] = df['Published Year'].iloc[i]\n",
    "            concatenated_adata.obs['PMID'] = df['PMID'].iloc[i]\n",
    "\n",
    "            # Save the concatenated AnnData object to the file\n",
    "            concatenated_adata.write_h5ad(output_path)\n",
    "            print(\"    File created successfully\")\n",
    "        else:\n",
    "            print(f\"The file made from {val} could not be created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered at index {i}: {e}\")\n",
    "\n",
    "print('All recipes complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d95af1-336d-48d7-870e-5ee4d98272cc",
   "metadata": {},
   "source": [
    "## Perform perturbation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c2462-5607-47cf-99b5-5d52837b1b5d",
   "metadata": {},
   "source": [
    "### Perturbation function definitions (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae9936-4f9c-4108-8cf4-64aca74682db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iterate_perturb_counts(adata, tf_list, scalar_list):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_counts(adata_temp, tf_list, scalar)\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n",
    "def perturb_counts(adata, tf_list, scalar): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['gene_symbol'].isin(tf_list)\n",
    "    \n",
    "    # Save the original state of the parameter objects, in case some tfs do not translate (failsafe)\n",
    "    original_X = adata.X.copy()\n",
    "    original_gene_mask = gene_mask.copy()\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    max_exp = np.max(adata.X, axis=1)\n",
    "\n",
    "    \"\"\"This is new today. v \"\"\"\n",
    "    # Raise an error if any of the gene names in tf_list do not match column names (we will manually update these in adata):\n",
    "    missing_genes = [gene for gene in tf_list if gene not in adata.var['gene_symbol'].values]\n",
    "    \n",
    "    if missing_genes:\n",
    "        # Restore original parameter objects\n",
    "        adata.X = original_X\n",
    "        gene_mask = original_gene_mask\n",
    "        raise ValueError(f\"Genes {missing_genes} not found in anndata object\")\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        # Apply the scaling operation to the specified genes\n",
    "        adata.X[:, gene_mask] = max_exp * scalar\n",
    "        \n",
    "        # Add/Update 'scaled' column in var\n",
    "        adata.var['scaled'] = gene_mask\n",
    "        \n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.var['scaled_by'] = scalar  # Default value for genes not in tf_list\n",
    "\n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.obs['U'] = scalar  # Default value for genes not in tf_list\n",
    "    # adata.var = \n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc240769-f15a-4086-9fb6-eacb0323041e",
   "metadata": {},
   "source": [
    "### Saving perturbations (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2df680-d997-4f01-8bd3-0cdefa87ccf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_directory = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/all-tfs\"\n",
    "scalars = [0.5, 0.75, 1.001, 1.25, 1.5]\n",
    "print(len(tf_ensid_list))\n",
    "\n",
    "tf_ensid_list = [id for id in tf_ensid_list if id not in problem_ens_ids]\n",
    "print(len(tf_ensid_list))\n",
    "\n",
    "for i in range(len(tf_ensid_list)):\n",
    "    val = df['Ensembl ID'].iloc[i]\n",
    "    # this handles the case where there is more than one tf to perturb in the same file, but in this case there is just the one\n",
    "    TFs = val.split()\n",
    "    # take out all the ensembl ids where there was no match in the couns matrix\n",
    "\n",
    "    if validate_ensembl_tfs(tf_ensid_list, counts_ensid_list):\n",
    "        # Join the TFs list into a string for the filename\n",
    "        TFs_str = \"_\".join(TFs)\n",
    "        \n",
    "        # Generate the file path for saving\n",
    "        file_name = f\"{TFs_str}.h5ad\"\n",
    "        output_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(output_path + \" already exists: continue!\")\n",
    "            continue\n",
    "        \n",
    "        print(TFs)\n",
    "        adataDict = iterate_perturb_counts(adata.copy(), TFs, scalars)\n",
    "\n",
    "        # Concatenate all AnnData objects along the observations axis\n",
    "        concatenated_adata = ad.concat(list(adataDict.values()), axis=0)\n",
    "\n",
    "        # This is the line was added today to fix the bug\n",
    "        concatenated_adata.var = adata.var.copy()\n",
    "\n",
    "        # Save reprogramming metadata into the concatenated_adata.obs table\n",
    "        concatenated_adata.obs['Source_cells'] = df['Source cells'].iloc[i]\n",
    "        concatenated_adata.obs['Target_cells'] = df['Target cells'].iloc[i]\n",
    "        concatenated_adata.obs['Treatment'] = df['Treatment'].iloc[i]\n",
    "        concatenated_adata.obs['Species'] = df['Species'].iloc[i]\n",
    "        concatenated_adata.obs['Cell_Transplantation'] = df['Cell Transplantation'].iloc[i]\n",
    "        concatenated_adata.obs['Published_Year'] = df['Published Year'].iloc[i]\n",
    "        concatenated_adata.obs['PMID'] = df['PMID'].iloc[i]\n",
    "        \n",
    "        # Save the concatenated AnnData object to the file\n",
    "        concatenated_adata.write_h5ad(output_path)\n",
    "        print(\"    file created\")\n",
    "    else:\n",
    "        print(\"The file made from \", val ,\" could not be created\")\n",
    "print('All recipes complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b933c-c244-4afe-bc09-1c2ee8307ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if TFs are in adata\n",
    "# missing_genes = [tf for tf in TFs if tf not in adata.var.index]\n",
    "# if missing_genes:\n",
    "#     print(f\"Missing genes in AnnData object: {missing_genes}\")\n",
    "\n",
    "adata.var.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c80541-cc8f-4536-8e52-f660945d8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tf_ensid_list)):\n",
    "    print(f\"Processing index {i}\")\n",
    "    val = df['Ensembl ID'].iloc[i]\n",
    "    TFs = val.split()\n",
    "    \n",
    "    print(f\"TFs: {TFs}\")\n",
    "    \n",
    "    tf_ensid_list_filtered = [id for id in problem_ens_ids if id not in tf_ensid_list]\n",
    "    \n",
    "    if not validate_ensembl_tfs(counts_ensid_list, tf_ensid_list_filtered):\n",
    "        print(\"Validation failed\")\n",
    "        continue\n",
    "    \n",
    "    TFs_str = \"_\".join(TFs)\n",
    "    file_name = f\"{TFs_str}.h5ad\"\n",
    "    output_path = os.path.join(output_directory, file_name)\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"{output_path} already exists\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Creating file: {output_path}\")\n",
    "    \n",
    "    adataDict = iterate_perturb_counts(adata.copy(), TFs, scalars)\n",
    "    concatenated_adata = ad.concat(list(adataDict.values()), axis=0)\n",
    "    concatenated_adata.var = adata.var.copy()\n",
    "    \n",
    "    concatenated_adata.obs['Source_cells'] = df['Source cells'].iloc[i]\n",
    "    concatenated_adata.obs['Target_cells'] = df['Target cells'].iloc[i]\n",
    "    concatenated_adata.obs['Treatment'] = df['Treatment'].iloc[i]\n",
    "    concatenated_adata.obs['Species'] = df['Species'].iloc[i]\n",
    "    concatenated_adata.obs['Cell_Transplantation'] = df['Cell Transplantation'].iloc[i]\n",
    "    concatenated_adata.obs['Published_Year'] = df['Published Year'].iloc[i]\n",
    "    concatenated_adata.obs['PMID'] = df['PMID'].iloc[i]\n",
    "    \n",
    "    concatenated_adata.write_h5ad(output_path)\n",
    "    print(f\"File created: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0fe09e-bee4-4666-9176-9282a8418120",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2570800-7f27-4621-910d-568cafd6ddbb",
   "metadata": {},
   "source": [
    "# Day 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0065148-4b7e-4a1d-98ad-b4f429e2029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Generating the perturbation files for all of the Human TFs (perturbation of 1, individually). \n",
    "Day 8 is designed for the paper spreadsheet, but some of the TFs column is still not formatted properly (not fixing this today)\n",
    "\"\"\"\n",
    "\"\"\" Important: Some of the same variable names are being used in the all tf experiment as in the paper experiment: both in the same situations, e.g. TFs is the TFs, but different ones between experiments\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba6f88-8b01-430c-99c8-d6d00bd7f083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bd3a9ca-d3a0-4351-a538-e64e270cbb0e",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad760306-773a-4bed-b655-8a8d2ac9dce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import scanpy as sp\n",
    "import os\n",
    "\n",
    "def iterate_perturb_counts(adata, tf_list, scalar_list):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_counts(adata_temp, tf_list, scalar)\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n",
    "def perturb_counts(adata, tf_list, scalar): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['gene_symbol'].isin(tf_list)\n",
    "    \n",
    "    # Save the original state of the parameter objects, in case some tfs do not translate (failsafe)\n",
    "    original_X = adata.X.copy()\n",
    "    original_gene_mask = gene_mask.copy()\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    max_exp = np.max(adata.X, axis=1)\n",
    "\n",
    "    \"\"\"This is new today. v \"\"\"\n",
    "    # Raise an error if any of the gene names in tf_list do not match column names (we will manually update these in adata):\n",
    "    missing_genes = [gene for gene in tf_list if gene not in adata.var['gene_symbol'].values]\n",
    "    \n",
    "    if missing_genes:\n",
    "        # Restore original parameter objects\n",
    "        adata.X = original_X\n",
    "        gene_mask = original_gene_mask\n",
    "        raise ValueError(f\"Genes {missing_genes} not found in anndata object\")\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        # Apply the scaling operation to the specified genes\n",
    "        adata.X[:, gene_mask] = max_exp * scalar\n",
    "        \n",
    "        # Add/Update 'scaled' column in var\n",
    "        adata.var['scaled'] = gene_mask\n",
    "        \n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.var['scaled_by'] = scalar  # Default value for genes not in tf_list\n",
    "\n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.obs['U'] = scalar  # Default value for genes not in tf_list\n",
    "    # adata.var = \n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4fba0-d09a-4989-b40d-e05d4a0533c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def translate_ids(list):\n",
    "    # Extract base IDs from list_b by removing version numbers\n",
    "    new_list = [id.split('.')[0] for id in list]\n",
    "    return new_list\n",
    "\n",
    "#ensembl ids from the counts matrix\n",
    "counts_ensid_list = adata.var['ensemblid'].values.tolist()\n",
    "counts_ensid_list = translate_ids(adata_id_list)\n",
    "\n",
    "\n",
    "# ensembl ids from the transcription factor list, which in this case is also the perturbation list (testing one at a time)\n",
    "tf_ensid_list = df['Ensembl ID'].values.tolist()\n",
    "\n",
    "\n",
    "def validate_ensembl_tfs(tf_ensid_list, counts_ensid_list):\n",
    "    for TF in ensembl_TFs:\n",
    "        if TF not in counts_ensid_list:\n",
    "            print(TF)\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "validate_ensembl_tfs(tf_ensid_list, counts_ensid_list)\n",
    "\n",
    "# when we perturb each tf in tf_ensid_list, we are just going to skip the one problematic one\n",
    "problem_ens_ids = ['ZNF73_HUMAN']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0944045-cfb8-4925-b809-54c45a530223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d81d2-8c5e-403a-9f01-fb6cc8d32626",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff13f1-e4e8-42dc-8a8d-ff0eb7a97b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reprogramming recipes\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Jpickard1/scFoundationModels/main/notebooks/reprogramming/data/HumanTFs_v_1.01.csv')\n",
    "df.head()\n",
    "\n",
    "# Load firboblast source cells\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/unperturbed\"\n",
    "FILE = \"fibroblast.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "\n",
    "adata.var.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff81e1-ddb8-47a2-bf7b-665329fbec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/one-shot/perturbed\"\n",
    "scalars = [0.5, 0.75, 1.001]\n",
    "for i in range(len(df['TFs'])):\n",
    "    val = df['TFs'].iloc[i]\n",
    "    TFs = val.split()\n",
    "    if validateTFs(TFs, adata):\n",
    "\n",
    "        # Join the TFs list into a string for the filename\n",
    "        TFs_str = \"_\".join(TFs)\n",
    "        \n",
    "        # Generate the file path for saving\n",
    "        file_name = f\"{TFs_str}.h5ad\"\n",
    "        output_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(output_path + \" already exist: continue!\")\n",
    "            continue\n",
    "        \n",
    "        print(TFs)\n",
    "        adataDict = iterate_perturb_counts(adata.copy(), TFs, scalars)\n",
    "\n",
    "        # Concatenate all AnnData objects along the observations axis\n",
    "        concatenated_adata = ad.concat(list(adataDict.values()), axis=0)\n",
    "\n",
    "        # This is the line was added today to fix the bug\n",
    "        concatenated_adata.var = adata.var.copy()\n",
    "\n",
    "        # Save reprogramming metadata into the concatenated_adata.obs table\n",
    "        concatenated_adata.obs['Source_cells'] = df['Source cells'].iloc[i]\n",
    "        concatenated_adata.obs['Target_cells'] = df['Target cells'].iloc[i]\n",
    "        concatenated_adata.obs['Treatment'] = df['Treatment'].iloc[i]\n",
    "        concatenated_adata.obs['Species'] = df['Species'].iloc[i]\n",
    "        concatenated_adata.obs['Cell_Transplantation'] = df['Cell Transplantation'].iloc[i]\n",
    "        concatenated_adata.obs['Published_Year'] = df['Published Year'].iloc[i]\n",
    "        concatenated_adata.obs['PMID'] = df['PMID'].iloc[i]\n",
    "        \n",
    "        # Save the concatenated AnnData object to the file\n",
    "        concatenated_adata.write_h5ad(output_path)\n",
    "        print(\"    file created\")\n",
    "\n",
    "print('All recipes complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506c304-f7a3-453e-9f2f-e5efd8bd0395",
   "metadata": {},
   "source": [
    "## Trash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febad736-4b90-4e8b-a2e7-dc601e528a49",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def validateTFs(TFs, adata):\n",
    "#     adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "#     for TF in TFs:\n",
    "#         if TF not in adata_gene_list:\n",
    "#             return False\n",
    "#     return True\n",
    "\n",
    "# # # to contrast the previous experiment, here genes are named by ensembl id rather than one of their names\n",
    "# ensembl_TFs = df['Ensembl ID']. values.tolist()\n",
    "# # def validate_ensembl_tfs(ensembl_TFs, adata):\n",
    "# #     adata_id_list = adata.var['ensemblid'].values.tolist()\n",
    "# #     for TF in ensembl_TFs:\n",
    "# #         if TF not in adata_id_list:\n",
    "# #             return False\n",
    "# #     return True\n",
    "\n",
    "# # validate_ensembl_tfs(ensembl_TFs, adata\n",
    "\n",
    "\n",
    "\n",
    "# def validate_ensembl_tfs(ensembl_TFs, new_adata_id_list):\n",
    "#     count = 0\n",
    "#     # Extract Ensembl IDs from the adata object\n",
    "\n",
    "#     # Initialize a flag to check if all IDs are found\n",
    "#     all_found = True\n",
    "#     # Iterate through the list of Ensembl IDs to validate\n",
    "#     for TF in ensembl_TFs:\n",
    "#         if TF not in adata_id_list:\n",
    "#             # Print the missing Ensembl ID\n",
    "#             #print(f\"{TF} not found in adata.var['ensemblid']\")\n",
    "#             count += 1\n",
    "#             # Set the flag to False if an ID is missing\n",
    "#             all_found = False\n",
    "#     # Return the final status of the validation\n",
    "#     print(len(adata_id_list))\n",
    "#     print(len(ensembl_TFs))\n",
    "#     print(count)\n",
    "#     return all_found\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8f1079-66fb-423f-9f56-e46d81c04d2a",
   "metadata": {},
   "source": [
    "# Day 8\n",
    "\n",
    "Generatting the perturbation files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff35e5-f45f-416f-95ca-313bd921c074",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6cd062-e13d-40a5-b679-1db068d463bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import scanpy as sp\n",
    "import os\n",
    "\n",
    "def iterate_perturb_counts(adata, tf_list, scalar_list):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_counts(adata_temp, tf_list, scalar)\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n",
    "def perturb_counts(adata, tf_list, scalar): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['gene_symbol'].isin(tf_list)\n",
    "    \n",
    "    # Save the original state of the parameter objects, in case some tfs do not translate (failsafe)\n",
    "    original_X = adata.X.copy()\n",
    "    original_gene_mask = gene_mask.copy()\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    max_exp = np.max(adata.X, axis=1)\n",
    "\n",
    "    \"\"\"This is new today. v \"\"\"\n",
    "    # Raise an error if any of the gene names in tf_list do not match column names (we will manually update these in adata):\n",
    "    missing_genes = [gene for gene in tf_list if gene not in adata.var['gene_symbol'].values]\n",
    "    \n",
    "    if missing_genes:\n",
    "        # Restore original parameter objects\n",
    "        adata.X = original_X\n",
    "        gene_mask = original_gene_mask\n",
    "        raise ValueError(f\"Genes {missing_genes} not found in anndata object\")\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        # Apply the scaling operation to the specified genes\n",
    "        adata.X[:, gene_mask] = max_exp * scalar\n",
    "        \n",
    "        # Add/Update 'scaled' column in var\n",
    "        adata.var['scaled'] = gene_mask\n",
    "        \n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.var['scaled_by'] = scalar  # Default value for genes not in tf_list\n",
    "\n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.obs['U'] = scalar  # Default value for genes not in tf_list\n",
    "    # adata.var = \n",
    "    return adata\n",
    "\n",
    "def validateTFs(TFs, adata):\n",
    "    adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "    for TF in TFs:\n",
    "        if TF not in adata_gene_list:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459f8409-257e-4c03-b216-62337293499c",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f639abb-5181-4d2b-89bc-e070a74af38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reprogramming recipes\n",
    "df = pd.read_csv('data/recipe_table_9_6_2024.csv')\n",
    "df.head()\n",
    "\n",
    "# Load firboblast source cells\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/unperturbed\"\n",
    "FILE = \"fibroblast.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844415e8-df65-428b-8d18-5601f57cd263",
   "metadata": {},
   "source": [
    "## Make Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c1a67-0ac0-4e94-b059-16eca9958c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/one-shot/perturbed\"\n",
    "scalars = [0.5, 0.75, 1.001]\n",
    "for i in range(len(df['TFs'])):\n",
    "    val = df['TFs'].iloc[i]\n",
    "    TFs = val.split()\n",
    "    if validateTFs(TFs, adata):\n",
    "\n",
    "        # Join the TFs list into a string for the filename\n",
    "        TFs_str = \"_\".join(TFs)\n",
    "        \n",
    "        # Generate the file path for saving\n",
    "        file_name = f\"{TFs_str}.h5ad\"\n",
    "        output_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(output_path + \" already exist: continue!\")\n",
    "            continue\n",
    "        \n",
    "        print(TFs)\n",
    "        adataDict = iterate_perturb_counts(adata.copy(), TFs, scalars)\n",
    "\n",
    "        # Concatenate all AnnData objects along the observations axis\n",
    "        concatenated_adata = ad.concat(list(adataDict.values()), axis=0)\n",
    "\n",
    "        # This is the line was added today to fix the bug\n",
    "        concatenated_adata.var = adata.var.copy()\n",
    "\n",
    "        # Save reprogramming metadata into the concatenated_adata.obs table\n",
    "        concatenated_adata.obs['Source_cells'] = df['Source cells'].iloc[i]\n",
    "        concatenated_adata.obs['Target_cells'] = df['Target cells'].iloc[i]\n",
    "        concatenated_adata.obs['Treatment'] = df['Treatment'].iloc[i]\n",
    "        concatenated_adata.obs['Species'] = df['Species'].iloc[i]\n",
    "        concatenated_adata.obs['Cell_Transplantation'] = df['Cell Transplantation'].iloc[i]\n",
    "        concatenated_adata.obs['Published_Year'] = df['Published Year'].iloc[i]\n",
    "        concatenated_adata.obs['PMID'] = df['PMID'].iloc[i]\n",
    "        \n",
    "        # Save the concatenated AnnData object to the file\n",
    "        concatenated_adata.write_h5ad(output_path)\n",
    "        print(\"    file created\")\n",
    "\n",
    "print('All recipes complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf4e3f-0f83-40d8-9a98-5c8e33a02d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TFs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa5087-b0ea-44c2-95c3-6bc2bec5ac89",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c74dbc-6b9f-443d-89fb-be4b752976f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42651dce-39f6-415a-a9a5-6d35695e1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05498f82-b879-4452-8d69-273a28b05d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "padata = perturb_counts(adata, ['DDX11L1'], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cdffdb-d65c-4162-b41c-15f9edfc526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_adata = ad.concat([adata, padata], axis=0)\n",
    "concatenated_adata.var = adata.var.copy()\n",
    "concatenated_adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2311423c-164d-4797-abcc-553f4016b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f591c5-ea99-4235-a2a6-9100fca8b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d79b7ad-c657-40c6-aaf0-9b80a1dc7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "padata.var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb287d2-59ac-4fbc-8887-8500217a591f",
   "metadata": {},
   "source": [
    "# Day 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982889a0-25b9-4fb4-8195-d3593c22ed67",
   "metadata": {},
   "source": [
    "# Day 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afe83b-fd13-436f-8dd6-1fd95fd883d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Changes to gene names and formatting are reflected in spreadsheet\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1edb3a-5d7c-4230-a2cb-5ec790935dd3",
   "metadata": {},
   "source": [
    "# Day 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa696c58-66b2-4eba-a1b1-b578eff3ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from a prev day\n",
    "\n",
    "\"\"\"\n",
    "Josh, please read: the adata.obs['scalar'] = scalar copies the scalar down for that call, associated with every cell in X. Same with ['scaled'] and ['scaled_by']\n",
    "in var. This is good in case the data is later appended into one anndata object.\n",
    "But my return from the perturb_counts loop (cell below this) is a dictionary of all of the perturb_counts, since appending along any axis will probably either overwrite\n",
    "obs or var.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def perturb_counts(tf_list, scalar, adata): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "    # Save the original state of the parameter objects, in case some tfs do not translate (failsafe)\n",
    "    original_X = adata.X.copy()\n",
    "    original_gene_mask = gene_mask.copy()\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    max_exp = np.max(adata.X, axis=1)\n",
    "    \n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['gene_symbol'].isin(tf_list)\n",
    "\n",
    "    \"\"\"This is new today. v \"\"\"\n",
    "    # Raise an error if any of the gene names in tf_list do not match column names (we will manually update these in adata):\n",
    "    missing_genes = [gene for gene in tf_list if gene not in adata.var['gene_symbol'].values]\n",
    "    \n",
    "    if missing_genes:\n",
    "        # Restore original parameter objects\n",
    "        adata.X = original_X\n",
    "        gene_mask = original_gene_mask\n",
    "        raise ValueError(f\"Genes {missing_genes} not found in anndata object\")\n",
    "\n",
    "    else:\n",
    "        \"\"\" This is new today. ^ \"\"\"\n",
    "        # Apply the scaling operation to the specified genes\n",
    "        adata.X[:, gene_mask] *= max_exp[:, np.newaxis] * scalar\n",
    "        \n",
    "        # Add/Update 'scaled' column in var\n",
    "        adata.var['scaled'] = gene_mask\n",
    "        \n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.var['scaled_by'] = 1  # Default value for genes not in tf_list\n",
    "        adata.var.loc[gene_mask, 'scaled_by'] = max_exp[:, np.newaxis] * scalar  # Correct scaling factor assignment\n",
    "    \n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a489b88-60e1-4387-8d03-20c129931da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from a prev day\n",
    "\n",
    "import anndata\n",
    "\n",
    "def iterate_perturb_counts(tf_list, scalar_list, adata):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_counts(tf_list, scalar, adata_temp)\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa93426a-3ff0-44e1-8b41-5d5e89d56666",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copied from a prev day\n",
    "\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "FILE = \"TS_epithelial.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "\n",
    "# Print the first 5 entries\n",
    "print(\"First 5 entries:\")\n",
    "print(adata_gene_list[:5])\n",
    "\n",
    "# Print the last 5 entries\n",
    "print(\"Last 5 entries:\")\n",
    "print(adata_gene_list[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270230bf-4742-48c8-90ab-0ba4ded0513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outside the function so I can manipulate these directly.\n",
    "# I also checked whether it matters if I do case insensitive (capitalize everything then compare) or case sensitive.\n",
    "# Unsurprisingly, case sensitive has more discrepancies (45 vs. 44), with the one extra that was picked up as \"Ptf1a\".\n",
    "# I left the case insensitive version. \n",
    "\n",
    "# get a list of words (potential genes, also includes and, + , etc. ) from the table from the review paper\n",
    "table_1_df = pd.read_csv(\"/home/oliven/scFoundationModels/notebooks/reprogramming/data/table_1_data_from_paper_9_1.csv\")\n",
    "combined_string = ' '.join(table_1_df['Treatment'].astype(str)).replace(',', '')\n",
    "word_list = combined_string.split()\n",
    "\n",
    "# get a list of genes that appear in the data matrix\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "FILE = \"TS_epithelial.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "\n",
    "# New today\n",
    "def check_valid_tfs(word_list, adata_gene_list):\n",
    "\n",
    "    word_list_upper = [word.upper() for word in word_list]\n",
    "    adata_gene_list_upper = [gene.upper() for gene in adata_gene_list]\n",
    "\n",
    "    # print what does not overlap\n",
    "    not_valid_gene = set(word_list_upper) - set(adata_gene_list_upper)\n",
    "\n",
    "    print(\"Entries in the table that are not genes in the counts matrix: \")\n",
    "    \n",
    "    return list(not_valid_gene)\n",
    "    \n",
    "check_valid_tfs(word_list, adata_gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930e2060-2c16-4f3b-9f64-cf1807778708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those genes with multiple aliases, checking which are valid\n",
    "# we have to worry about making this case insensitive just in case\n",
    "multiple_alias_dict = {}\n",
    "multiple_alias_dict.update({\n",
    "    'P53': ['BCC7','BMFS5', 'LFS1', 'TRP53'],\n",
    "    'OCT3/4': ['POU5F1', 'OCT3', 'OCT4', 'OTF4', 'MGC22487'],\n",
    "    'MASH1': ['HASH1', 'BHLHa46', 'ASH1', 'ASH-1', 'ASCL1'],\n",
    "    'HNF6': ['HNF6', 'HNF6A', 'ONECUT1'],\n",
    "    'HB9': ['MNX1', 'HOXHB9', 'SCRA1', 'HLXB9', 'GC07M156491', 'GC07M156786', 'GC07M150530'],\n",
    "    'PPARG2': ['PPARG', 'NR1C3', 'PPARG1', 'PPARgamma', 'PPAR-Gamma', 'PPARG5', 'CIMT1', 'GLM1'], \n",
    "    'PU.1': ['SPI1', 'SPI-A', 'SFPI1', 'SPI-1', 'OF', 'AGM10'],\n",
    "    'N-MYC': ['MYCN', 'BHLHe37', 'N-Myc', 'NMYC', 'MYCNOT', 'MYCNsORF', 'MYCNsPEP', 'BHLHE37', 'FGLDS1', 'MODED', 'MPAPA', 'ODED'],\n",
    "    # oct9 had a strange genecards lookup\n",
    "    'OCT9': ['POU3F4', 'SLC22A16'],\n",
    "    'LEF-1': ['TCF1ALPHA', 'TCF7L3', 'TCF10', 'LEF1'],\n",
    "    # these next two were listed as ER71/ETV2\n",
    "    'ER71/ETV2': ['ER71', 'ETV2', 'ETSRP71'],\n",
    "    # sv40 had a strange genecards lookup\n",
    "    'SV40': [''],\n",
    "    # this one didn't show up\n",
    "    'LXH3': ['M2-LHX3', 'M2LHX3', 'CPHD3', 'LIM3'],\n",
    "    'NGN2': ['NEUROG2', 'BHLHA8', 'MATH4A', 'Math4a', 'ATOH4', 'Ngn-2', 'BHLHA8', 'Atoh4', 'NGN-2'],\n",
    "    'LMX1A': ['LMX1.1', 'LMX1', 'LMX-1.', 'DFNA7'],\n",
    "    # NF-Kb had  a strange genecards lookup\n",
    "    'NF-B': ['NFkb1', 'NFKB1'], \n",
    "    'L-MYC': ['MYCL', 'LMYC', 'BHLHe38', 'MYCL1', 'BHLHE38'],\n",
    "    'BRN2': ['POU3F2', 'BRN2', 'OCT7', 'POUF3', 'OTF7', 'Brain-2', 'OTF-7', 'Brn-2', 'Oct-7', 'N-Oct3'], \n",
    "    'NURR1': ['NR4A2', 'TINUR', 'NOT', 'HZF3', 'NURR1', 'RNR1', 'IDLDP'],\n",
    "    'SOX2': ['SRY-Box 2', 'MCOPS3', 'ANOP3'],\n",
    "    'NEUROD': ['NEUROD1', 'BHLHa3', 'BETA2', 'BHF-1', 'MODY6', 'NeuroD1', 'BHLHA3', 'T2D'],\n",
    "    'C-MYC': ['MYC', 'C-MYC', 'MYCC', 'MRTL', 'BHLHE39'],\n",
    "    # AP-2A had  a strange genecards lookup\n",
    "    'AP-2A': [' '],\n",
    "    'PAX6': ['D11S812E', 'WAGR', 'AN2', 'AN', 'AN1', 'ASGD5', 'FVH1', 'MGDA'],\n",
    "    'OSTERIX': ['SP7', 'OSX', 'OI11', 'OI12'],\n",
    "    # OCT6 had a strange genecards lookup\n",
    "    'OCT6': ['POU3F1', 'SCIP', 'OTF6', 'OTF-6'],\n",
    "   \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed97378-5da6-46b7-9b51-d1b8fca15442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2881f1-9966-4946-a2c4-d27fb6de066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day 4, technically (migrate this)\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "def identify_gene_name_translations(multiple_alias_dict, adata_gene_list):\n",
    "    \"\"\"\n",
    "    Identifies whether any values from multiple_alias_dict appear in adata_gene_list, case-insensitive.\n",
    "    \n",
    "    Parameters:\n",
    "    multiple_alias_dict (dict): A dictionary where keys are gene names and values are lists of aliases.\n",
    "    adata_gene_list (list): A list of gene names to check against, case-insensitive.\n",
    "\n",
    "    Prints:\n",
    "    For each key, whether it was found in the gene list along with the matching values.\n",
    "    \"\"\"\n",
    "    # Convert the gene list to uppercase for case-insensitive comparison\n",
    "    adata_gene_list_upper = [gene.upper() for gene in adata_gene_list]\n",
    "\n",
    "    # Loop through each key and values in the dictionary\n",
    "    for key, values in multiple_alias_dict.items():\n",
    "        # Convert each alias to uppercase\n",
    "        values_upper = [value.upper() for value in values]\n",
    "        \n",
    "        # Check if any alias is present in the gene list\n",
    "        found_values = [value for value in values_upper if value in adata_gene_list_upper]\n",
    "        \n",
    "        # Print appropriate message based on whether any values were found\n",
    "        if found_values:\n",
    "            print(f\"{key} was found in gene list as {found_values}\")\n",
    "        else:\n",
    "            print(f\"{key} was not found in gene list.\")\n",
    "\n",
    "identify_gene_name_translations(multiple_alias_dict, adata_gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5db023-1ae7-46cf-a4d8-7a9eaf5b193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba1127-ee3c-478f-8298-4a738d540bc6",
   "metadata": {},
   "source": [
    "# Day 4: Mine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c0cbf-311c-4de6-9e83-8dae191d7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copied from a prev day\n",
    "\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "FILE = \"TS_epithelial.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "\n",
    "# Print the first 5 entries\n",
    "print(\"First 5 entries:\")\n",
    "print(adata_gene_list[:5])\n",
    "\n",
    "# Print the last 5 entries\n",
    "print(\"Last 5 entries:\")\n",
    "print(adata_gene_list[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee05aa9b-e46f-4a68-a047-704b9fb4effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outside the function so I can manipulate these directly.\n",
    "# I also checked whether it matters if I do case insensitive (capitalize everything then compare) or case sensitive.\n",
    "# Unsurprisingly, case sensitive has more discrepancies (45 vs. 44), with the one extra that was picked up as \"Ptf1a\".\n",
    "# I left the case insensitive version. \n",
    "\n",
    "# get a list of words (potential genes, also includes and, + , etc. ) from the table from the review paper\n",
    "table_1_df = pd.read_csv(\"/home/oliven/scFoundationModels/notebooks/reprogramming/data/table_1_data_from_paper_9_1.csv\")\n",
    "combined_string = ' '.join(table_1_df['TFs'].astype(str)).replace(',', '')\n",
    "word_list = combined_string.split()\n",
    "\n",
    "# get a list of genes that appear in the data matrix\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "FILE = \"TS_epithelial.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "\n",
    "# New today\n",
    "def check_valid_tfs(word_list, adata_gene_list):\n",
    "\n",
    "    word_list_upper = [word.upper() for word in word_list]\n",
    "    adata_gene_list_upper = [gene.upper() for gene in adata_gene_list]\n",
    "\n",
    "    # print what does not overlap\n",
    "    not_valid_gene = set(word_list_upper) - set(adata_gene_list_upper)\n",
    "\n",
    "    print(\"Entries in the table that are not genes in the counts matrix: \")\n",
    "    \n",
    "    return list(not_valid_gene)\n",
    "    \n",
    "check_valid_tfs(word_list, adata_gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d64c66-8ea7-4ea6-928e-b907f456735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those genes with multiple aliases, checking which are valid\n",
    "# we have to worry about making this case insensitive just in case\n",
    "multiple_alias_dict = {}\n",
    "multiple_alias_dict.update({\n",
    "    'p53': ['BCC7','BMFS5', 'LFS1', 'TRP53'],\n",
    "    'OCT3/4': ['POU5F1', 'OCT3', 'OCT4', 'OTF4', 'MGC22487'],\n",
    "    'MASH1': ['HASH1', 'BHLHa46', 'ASH1', 'ASH-1', 'ASCL1'],\n",
    "    'HNF6': ['HNF6', 'HNF6A', 'ONECUT1'],\n",
    "    'HB9': ['MNX1', 'HOXHB9', 'SCRA1', 'HLXB9', 'GC07M156491', 'GC07M156786', 'GC07M150530'],\n",
    "    'PPARG2': ['PPARG', 'NR1C3', 'PPARG1', 'PPARgamma', 'PPAR-Gamma', 'PPARG5', 'CIMT1', 'GLM1'], \n",
    "    'PU.1': ['SPI1', 'SPI-A', 'SFPI1', 'SPI-1', 'OF', 'AGM10'],\n",
    "    'N-MYC': ['MYCN', 'BHLHe37', 'N-Myc', 'NMYC', 'MYCNOT', 'MYCNsORF', 'MYCNsPEP', 'BHLHE37', 'FGLDS1', 'MODED', 'MPAPA', 'ODED'],\n",
    "    # oct9 had a strange genecards lookup\n",
    "    'OCT9': ['POU3F4', 'SLC22A16']\n",
    "    'LEF-1': ['TCF1ALPHA', 'TCF7L3', 'TCF10', 'LEF1'],\n",
    "    # these next two were listed as ER71/ETV2\n",
    "    'ER71/ETV2': ['ER71', 'ETV2', 'ETSRP71'],\n",
    "    # sv40 had a strange genecards lookup\n",
    "    'SV40': [''],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932fa31-967f-4f7f-ab6b-845791e191c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0183d92-2ee0-4b6f-a711-9cdc55a23260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day 4, technically (migrate this)\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "def identify_gene_name_translations(multiple_alias_dict, adata_gene_list):\n",
    "    \"\"\"\n",
    "    Identifies whether any values from multiple_alias_dict appear in adata_gene_list, case-insensitive.\n",
    "    \n",
    "    Parameters:\n",
    "    multiple_alias_dict (dict): A dictionary where keys are gene names and values are lists of aliases.\n",
    "    adata_gene_list (list): A list of gene names to check against, case-insensitive.\n",
    "\n",
    "    Prints:\n",
    "    For each key, whether it was found in the gene list along with the matching values.\n",
    "    \"\"\"\n",
    "    # Convert the gene list to uppercase for case-insensitive comparison\n",
    "    adata_gene_list_upper = [gene.upper() for gene in adata_gene_list]\n",
    "\n",
    "    # Loop through each key and values in the dictionary\n",
    "    for key, values in multiple_alias_dict.items():\n",
    "        # Convert each alias to uppercase\n",
    "        values_upper = [value.upper() for value in values]\n",
    "        \n",
    "        # Check if any alias is present in the gene list\n",
    "        found_values = [value for value in values_upper if value in adata_gene_list_upper]\n",
    "        \n",
    "        # Print appropriate message based on whether any values were found\n",
    "        if found_values:\n",
    "            print(f\"{key} was found in gene list as {found_values}\")\n",
    "        else:\n",
    "            print(f\"{key} was not found in gene list.\")\n",
    "\n",
    "identify_gene_name_translations(multiple_alias_dict, adata_gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e04100-5836-4bb5-985b-252c739d7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above list is small enough that I can manually check it.\n",
    "\n",
    "# Things to remove from word_list:\n",
    "not_genes = ['Variant', 'Large',  '(ETS', '2)', 'Knockdown', ]\n",
    "\n",
    "# Valid genes to replace/rename in word_list:\n",
    "genes_to_translate = ['LMX1A;', #'P53']\n",
    "translated_names = ['LMX1A',]\n",
    "\n",
    "# These ones might have appeared as multiple entries, etc. b/c of spacing. easiest way was to delete and add back\n",
    "genes_to_add = ['ETS2',]\n",
    "\n",
    "# replace then subtract and add. [--------------]\n",
    "new_word_list = set(word_list) - set(not_genes)\n",
    "\n",
    "# Running the function one more time to check:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b55c4-7e15-47c1-8173-4859a1a32303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Renamed med_nonz to max_exp to be more accurate. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1607b7d-0394-4092-b3d7-1f5337304ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from a prev day\n",
    "\n",
    "\"\"\"\n",
    "Josh, please read: the adata.obs['scalar'] = scalar copies the scalar down for that call, associated with every cell in X. Same with ['scaled'] and ['scaled_by']\n",
    "in var. This is good in case the data is later appended into one anndata object.\n",
    "But my return from the perturb_counts loop (cell below this) is a dictionary of all of the perturb_counts, since appending along any axis will probably either overwrite\n",
    "obs or var.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def perturb_counts(tf_list, scalar, adata): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "    # Save the original state of the parameter objects, in case some tfs do not translate (failsafe)\n",
    "    original_X = adata.X.copy()\n",
    "    original_gene_mask = gene_mask.copy()\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    max_exp = np.max(adata.X, axis=1)\n",
    "    \n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['gene_symbol'].isin(tf_list)\n",
    "\n",
    "    \"\"\"This is new today. v \"\"\"\n",
    "    # Raise an error if any of the gene names in tf_list do not match column names (we will manually update these in adata):\n",
    "    missing_genes = [gene for gene in tf_list if gene not in adata.var['gene_symbol'].values]\n",
    "    \n",
    "    if missing_genes:\n",
    "        # Restore original parameter objects\n",
    "        adata.X = original_X\n",
    "        gene_mask = original_gene_mask\n",
    "        raise ValueError(f\"Genes {missing_genes} not found in anndata object\")\n",
    "\n",
    "    else:\n",
    "        \"\"\" This is new today. ^ \"\"\"\n",
    "        # Apply the scaling operation to the specified genes\n",
    "        adata.X[:, gene_mask] *= max_exp[:, np.newaxis] * scalar\n",
    "        \n",
    "        # Add/Update 'scaled' column in var\n",
    "        adata.var['scaled'] = gene_mask\n",
    "        \n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.var['scaled_by'] = 1  # Default value for genes not in tf_list\n",
    "        adata.var.loc[gene_mask, 'scaled_by'] = max_exp[:, np.newaxis] * scalar  # Correct scaling factor assignment\n",
    "    \n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a083ac-8ec8-4b5e-8084-3299bf915df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from a prev day\n",
    "\n",
    "import anndata\n",
    "\n",
    "def iterate_perturb_counts(tf_list, scalar_list, adata):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_counts(tf_list, scalar, adata_temp)\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d543ea4-8e81-4a18-8a6a-9d58cca3794f",
   "metadata": {},
   "source": [
    "# Day 4: Copied From Josh's Notebook\n",
    "\n",
    "**Focus:** check out Nats code (debug a bit) and create a few perturbations\n",
    "- changes made to NO's code:\n",
    "    1. iterate_perturb_counts: changes the order of the arguments to `adata, tf_list, scalar_list`\n",
    "    2. perturb_counts: changes the order of the arguments to `adata, tf_list, scalar_list`\n",
    "    3. perturb_counts: there was an issue with the use of `[: np.newaxis]` with respect to `max_exp`, which is a `coo_matrix` (special type of sparse matrix). Code was modified to address an issue being thrown here.\n",
    "- new function:\n",
    "    1. validateTFs(TFs, adata): this checks if all the transcription factors are present in the adata\n",
    "- pertrubation driver (`Perform Perturbations and Create new files`):\n",
    "    1. loads Fibroblast data from Tabula Sapiens\n",
    "    2. loads `.csv` file of known reprogrmaming protocols\n",
    "    3. for each set of TFs that are validated by `validateTFs`:\n",
    "        1. use `iterate_perturb_counts` to generate perturbations with scalars `[0.5, 0.75, 1]`\n",
    "        2. concatenate the dataframes to make a single dataframe\n",
    "        3. save metadata from reprogramming protocol (i.e. PMID, source/targets, etc.)\n",
    "        4. save the new anndata as a `.h5ad` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd7570a-143e-42ce-9da0-092a30f2a5de",
   "metadata": {},
   "source": [
    "## Nat's Code with some modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b62fc-2de4-4c72-989b-dd5b2c2447ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import scanpy as sp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74e22ec-6676-4879-94a7-7ecdf360fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_perturb_counts(adata, tf_list, scalar_list):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_counts(adata_temp, tf_list, scalar)\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n",
    "def perturb_counts(adata, tf_list, scalar): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['gene_symbol'].isin(tf_list)\n",
    "    \n",
    "    # Save the original state of the parameter objects, in case some tfs do not translate (failsafe)\n",
    "    original_X = adata.X.copy()\n",
    "    original_gene_mask = gene_mask.copy()\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    max_exp = np.max(adata.X, axis=1)\n",
    "\n",
    "    \"\"\"This is new today. v \"\"\"\n",
    "    # Raise an error if any of the gene names in tf_list do not match column names (we will manually update these in adata):\n",
    "    missing_genes = [gene for gene in tf_list if gene not in adata.var['gene_symbol'].values]\n",
    "    \n",
    "    if missing_genes:\n",
    "        # Restore original parameter objects\n",
    "        adata.X = original_X\n",
    "        gene_mask = original_gene_mask\n",
    "        raise ValueError(f\"Genes {missing_genes} not found in anndata object\")\n",
    "\n",
    "    else:    \n",
    "        \n",
    "        # Apply the scaling operation to the specified genes\n",
    "        adata.X[:, gene_mask] = max_exp * scalar\n",
    "        \n",
    "        # Add/Update 'scaled' column in var\n",
    "        adata.var['scaled'] = gene_mask\n",
    "        \n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.var['scaled_by'] = scalar  # Default value for genes not in tf_list\n",
    "    \n",
    "    return adata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5645f9-2865-4256-9a67-16714be224ba",
   "metadata": {},
   "source": [
    "## New code to validate lists of TFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458b5b2-728b-4868-91ac-dd439523a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validateTFs(TFs, adata):\n",
    "    adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "    for TF in TFs:\n",
    "        if TF not in adata_gene_list:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf8a3e-cbf6-4a03-810d-52d07b426076",
   "metadata": {},
   "source": [
    "## Load data and perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2da050-9ac3-4a40-82fe-ff3c9f125689",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/first_5_recepies_8_29_2024.csv')\n",
    "\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/jpic/\"\n",
    "FILE = \"fibroblast.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754f880-759d-4ceb-ad5e-c7aec8535f99",
   "metadata": {},
   "source": [
    "## Perform Perturbations and Create new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf265d-6a26-476b-826a-08997f1c40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['TFs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c916f-8cca-4155-a81b-318d253ca6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = \"/nfs/turbo/umms-indikar/shared/projects/DARPA_AI/in-silico-reprogramming/one-shot/perturbed\"\n",
    "scalars = [0.5, 0.75, 1.001]\n",
    "for i in range(len(df['TFs'])):\n",
    "    val = df['TFs'].iloc[i]\n",
    "    val = val.replace(',',' ')\n",
    "    val = val.replace(';',' ')\n",
    "    val = val.replace(':',' ')\n",
    "    TFs = val.split(' ')\n",
    "    if validateTFs(TFs, adata):\n",
    "        print(TFs)\n",
    "        adataDict = iterate_perturb_counts(adata, TFs, scalars)\n",
    "\n",
    "        # Concatenate all AnnData objects along the observations axis\n",
    "        concatenated_adata = ad.concat(list(adataDict.values()), axis=0)\n",
    "\n",
    "        # Save reprogramming metadata into the concatenated_adata.obs table\n",
    "        concatenated_adata.obs['Source_cells'] = df['Source cells'].iloc[i]\n",
    "        concatenated_adata.obs['Target_cells'] = df['Target cells'].iloc[i]\n",
    "        concatenated_adata.obs['Treatment'] = df['Treatment'].iloc[i]\n",
    "        concatenated_adata.obs['Species'] = df['Species'].iloc[i]\n",
    "        concatenated_adata.obs['Cell_Transplantation'] = df['Cell Transplantation'].iloc[i]\n",
    "        concatenated_adata.obs['Published_Year'] = df['Published Year'].iloc[i]\n",
    "        concatenated_adata.obs['PMID'] = df['PMID'].iloc[i]\n",
    "        \n",
    "        # Join the TFs list into a string for the filename\n",
    "        TFs_str = \"_\".join(TFs)\n",
    "        \n",
    "        # Generate the file path for saving\n",
    "        file_name = f\"{TFs_str}.h5ad\"\n",
    "        output_path = os.path.join(output_directory, file_name)\n",
    "        \n",
    "        # Save the concatenated AnnData object to the file\n",
    "        concatenated_adata.write_h5ad(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a3a04e-5927-424b-9a17-fb698e0d5d4b",
   "metadata": {},
   "source": [
    "# Day 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae743d0-4445-45da-aa67-60dbfd13aa6e",
   "metadata": {},
   "source": [
    "# Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca734bb-76c1-453b-b0bb-e2caaff6be0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copied from a prev day\n",
    "\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "FILE = \"TS_epithelial.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "\n",
    "# Print the first 5 entries\n",
    "print(\"First 5 entries:\")\n",
    "print(adata_gene_list[:5])\n",
    "\n",
    "# Print the last 5 entries\n",
    "print(\"Last 5 entries:\")\n",
    "print(adata_gene_list[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44c1301-ecaa-40f0-bcc4-aba1e08e04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outside the function so I can manipulate these directly.\n",
    "# I also checked whether it matters if I do case insensitive (capitalize everything then compare) or case sensitive.\n",
    "# Unsurprisingly, case sensitive has more discrepancies (45 vs. 44), with the one extra that was picked up as \"Ptf1a\".\n",
    "# I left the case insensitive version. \n",
    "\n",
    "# get a list of words (potential genes, also includes and, + , etc. ) from the table from the review paper\n",
    "table_1_df = pd.read_csv(\"/home/oliven/scFoundationModels/notebooks/reprogramming/data/table_1_data_from_paper_9_1.csv\")\n",
    "combined_string = ' '.join(table_1_df['TFs'].astype(str)).replace(',', '')\n",
    "word_list = combined_string.split()\n",
    "\n",
    "# get a list of genes that appear in the data matrix\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "FILE = \"TS_epithelial.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "\n",
    "# New today\n",
    "def check_valid_tfs(word_list, adata_gene_list):\n",
    "\n",
    "    word_list_upper = [word.upper() for word in word_list]\n",
    "    adata_gene_list_upper = [gene.upper() for gene in adata_gene_list]\n",
    "\n",
    "    # print what does not overlap\n",
    "    not_valid_gene = set(word_list_upper) - set(adata_gene_list_upper)\n",
    "\n",
    "    print(\"Entries in the table that are not genes in the counts matrix: \")\n",
    "    \n",
    "    return list(not_valid_gene)\n",
    "    \n",
    "check_valid_tfs(word_list, adata_gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62011aea-3548-4942-847a-d17616055966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for those genes with multiple aliases, checking which are valid\n",
    "# we have to worry about making this case insensitive just in case\n",
    "multiple_alias_dict = {}\n",
    "multiple_alias_dict.update({\n",
    "    'p53': ['BCC7','BMFS5', 'LFS1', 'TRP53'],\n",
    "    'OCT3/4': ['POU5F1', 'OCT3', 'OCT4', 'OTF4', 'MGC22487'],\n",
    "    'MASH1': ['HASH1', 'BHLHa46', 'ASH1', 'ASH-1', 'ASCL1'],\n",
    "    'HNF6': ['HNF6', 'HNF6A', 'ONECUT1'],\n",
    "    'HB9': ['MNX1', 'HOXHB9', 'SCRA1', 'HLXB9', 'GC07M156491', 'GC07M156786', 'GC07M150530'],\n",
    "    'PPARG2': ['PPARG', 'NR1C3', 'PPARG1', 'PPARgamma', 'PPAR-Gamma', 'PPARG5', 'CIMT1', 'GLM1'], \n",
    "    'PU.1': ['SPI1', 'SPI-A', 'SFPI1', 'SPI-1', 'OF', 'AGM10'],\n",
    "    'N-MYC': ['MYCN', 'BHLHe37', 'N-Myc', 'NMYC', 'MYCNOT', 'MYCNsORF', 'MYCNsPEP', 'BHLHE37', 'FGLDS1', 'MODED', 'MPAPA', 'ODED'],\n",
    "    # oct9 had a strange genecards lookup\n",
    "    'OCT9': ['POU3F4', 'SLC22A16']\n",
    "    'LEF-1': ['TCF1ALPHA', 'TCF7L3', 'TCF10', 'LEF1'],\n",
    "    # these next two were listed as ER71/ETV2\n",
    "    'ER71/ETV2': ['ER71', 'ETV2', 'ETSRP71'],\n",
    "    # sv40 had a strange genecards lookup\n",
    "    'SV40': [''],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f439e6-738c-4cba-8067-fc8a28af8a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1df6ef-73f6-4d86-8dfe-335d4a067b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# day 4, technically (migrate this)\n",
    "adata_gene_list = adata.var['gene_symbol'].values.tolist()\n",
    "def identify_gene_name_translations(multiple_alias_dict, adata_gene_list):\n",
    "    \"\"\"\n",
    "    Identifies whether any values from multiple_alias_dict appear in adata_gene_list, case-insensitive.\n",
    "    \n",
    "    Parameters:\n",
    "    multiple_alias_dict (dict): A dictionary where keys are gene names and values are lists of aliases.\n",
    "    adata_gene_list (list): A list of gene names to check against, case-insensitive.\n",
    "\n",
    "    Prints:\n",
    "    For each key, whether it was found in the gene list along with the matching values.\n",
    "    \"\"\"\n",
    "    # Convert the gene list to uppercase for case-insensitive comparison\n",
    "    adata_gene_list_upper = [gene.upper() for gene in adata_gene_list]\n",
    "\n",
    "    # Loop through each key and values in the dictionary\n",
    "    for key, values in multiple_alias_dict.items():\n",
    "        # Convert each alias to uppercase\n",
    "        values_upper = [value.upper() for value in values]\n",
    "        \n",
    "        # Check if any alias is present in the gene list\n",
    "        found_values = [value for value in values_upper if value in adata_gene_list_upper]\n",
    "        \n",
    "        # Print appropriate message based on whether any values were found\n",
    "        if found_values:\n",
    "            print(f\"{key} was found in gene list as {found_values}\")\n",
    "        else:\n",
    "            print(f\"{key} was not found in gene list.\")\n",
    "\n",
    "identify_gene_name_translations(multiple_alias_dict, adata_gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac00a2c8-771e-463a-bdbb-3668d31b6868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above list is small enough that I can manually check it.\n",
    "\n",
    "# Things to remove from word_list:\n",
    "not_genes = ['Variant', 'Large',  '(ETS', '2)', 'Knockdown', ]\n",
    "\n",
    "# Valid genes to replace/rename in word_list:\n",
    "genes_to_translate = ['LMX1A;', #'P53']\n",
    "translated_names = ['LMX1A',]\n",
    "\n",
    "# These ones might have appeared as multiple entries, etc. b/c of spacing. easiest way was to delete and add back\n",
    "genes_to_add = ['ETS2',]\n",
    "\n",
    "# replace then subtract and add. [--------------]\n",
    "new_word_list = set(word_list) - set(not_genes)\n",
    "\n",
    "# Running the function one more time to check:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bed245-f122-47cd-bc76-814e682c23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Renamed med_nonz to max_exp to be more accurate. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee7615-c3ae-4b05-a473-e3274728c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from a prev day\n",
    "\n",
    "\"\"\"\n",
    "Josh, please read: the adata.obs['scalar'] = scalar copies the scalar down for that call, associated with every cell in X. Same with ['scaled'] and ['scaled_by']\n",
    "in var. This is good in case the data is later appended into one anndata object.\n",
    "But my return from the perturb_counts loop (cell below this) is a dictionary of all of the perturb_counts, since appending along any axis will probably either overwrite\n",
    "obs or var.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def perturb_counts(tf_list, scalar, adata): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "    # Save the original state of the parameter objects, in case some tfs do not translate (failsafe)\n",
    "    original_X = adata.X.copy()\n",
    "    original_gene_mask = gene_mask.copy()\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    max_exp = np.max(adata.X, axis=1)\n",
    "    \n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['gene_symbol'].isin(tf_list)\n",
    "\n",
    "    \"\"\"This is new today. v \"\"\"\n",
    "    # Raise an error if any of the gene names in tf_list do not match column names (we will manually update these in adata):\n",
    "    missing_genes = [gene for gene in tf_list if gene not in adata.var['gene_symbol'].values]\n",
    "    \n",
    "    if missing_genes:\n",
    "        # Restore original parameter objects\n",
    "        adata.X = original_X\n",
    "        gene_mask = original_gene_mask\n",
    "        raise ValueError(f\"Genes {missing_genes} not found in anndata object\")\n",
    "\n",
    "    else:\n",
    "        \"\"\" This is new today. ^ \"\"\"\n",
    "        # Apply the scaling operation to the specified genes\n",
    "        adata.X[:, gene_mask] *= max_exp[:, np.newaxis] * scalar\n",
    "        \n",
    "        # Add/Update 'scaled' column in var\n",
    "        adata.var['scaled'] = gene_mask\n",
    "        \n",
    "        # Add/Update 'scaled_by' column in var\n",
    "        adata.var['scaled_by'] = 1  # Default value for genes not in tf_list\n",
    "        adata.var.loc[gene_mask, 'scaled_by'] = max_exp[:, np.newaxis] * scalar  # Correct scaling factor assignment\n",
    "    \n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeb06cf-f71b-4c2c-b8a2-9d7b81346258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from a prev day\n",
    "\n",
    "import anndata\n",
    "\n",
    "def iterate_perturb_counts(tf_list, scalar_list, adata):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_counts(tf_list, scalar, adata_temp)\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd82877-b10f-48f4-894a-ebdf8117d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want each recipe (returned as a dictionary of anndata objects, one adata object for each scalar)\n",
    "def save_perturb_to_turbo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0a638-b15e-4589-94d6-0275ada43a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing on one of the tf lists from the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad2d9cc-78c2-4d3c-a3c2-b0710a27000c",
   "metadata": {},
   "source": [
    "# Day 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb10707-df05-4e7d-a9a4-bd301ab5cd69",
   "metadata": {},
   "source": [
    "## Perturbation Model Discussion\n",
    "\n",
    "E.V. = expression values\n",
    "\n",
    "Possible algorithm:\n",
    "```\n",
    "1. find highest E.V.  for a single cell\n",
    "2. find expression value of TFs being modified\n",
    "3. have a value k for the number of different concentrations we want to test\n",
    "4. choose k different amounts to increase the TFs from there measured E.V. to the 150% maximum E.V.\n",
    "   - make an arbitray choice and code it up\n",
    "```\n",
    "\n",
    "**A reasonable person could write this 10s of different ways**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206882c6-cec6-4a8b-9605-bfdc8b058823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# median nonzero value of each row\n",
    "# the w stands for working. I just dont want to screw up the original.\n",
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "FILE = \"TS_epithelial.h5ad\"\n",
    "adata_w = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "adata_w\n",
    "# def median_nonzero(col):\n",
    "#     nonzero_vals = col[col != 0]  # Extract nonzero values\n",
    "#     if len(nonzero_vals) == 0:    # If no nonzero values, return NaN\n",
    "#         return np.nan\n",
    "#     return np.median(nonzero_vals)\n",
    "\n",
    "# # Apply the function to each column and store the results\n",
    "# med_nonz = np.apply_along_axis(median_nonzero, axis=0, arr=adata_w.X)\n",
    "# adata_w.var['med_nonz'] = med_nonz\n",
    "# adata_w.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d3635-dfd9-4412-9d95-cfcbfdf0fcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X = adata_w.X.toarray() if not isinstance(adata_w.X, np.ndarray) else adata_w.X\n",
    "\n",
    "def median_nonzero(col):\n",
    "    nonzero_vals = col[col != 0] \n",
    "    return np.median(nonzero_vals) if len(nonzero_vals) > 0 else 0\n",
    "\n",
    "#perform and save results of fn\n",
    "med_nonz = np.apply_along_axis(median_nonzero, axis=0, arr=X)\n",
    "adata_w.var['med_nonz'] = med_nonz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06553f13-9b94-4a5f-9039-bcf183b14ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_w.var.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2c287-8651-4280-b369-60bcae264beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeing what it looks like before tf_list changes the first 3 rows\n",
    "# Convert to dense if it's sparse and display the first five rows\n",
    "import numpy as np\n",
    "\n",
    "# Convert to a dense array if necessary\n",
    "dense_X = adata_w.X.toarray() if not isinstance(adata_w.X, np.ndarray) else adata_w.X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621349c1-2e81-4842-b701-ef86075bebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem, scaling by a factor of the max expressed gene in that cell means that you could be scaling by different genes for each cell,\n",
    "# when the cells are all of the same type. for each get the median nonzero expression\n",
    "\n",
    "# for testing purposes: v\n",
    "tf_list = ['DDX11L1', 'WASH7P', 'MIR6859-1']\n",
    "tf = 'DDX11L1'\n",
    "# for testing purposes: ^\n",
    "\n",
    "#mask = adata_w.obs_names.isin(tf_list)\n",
    "mask = np.where(adata_w.var['gene_symbol'] == tf)[0]\n",
    "adata_w.X[mask, :] = adata_w.X[mask, :] * adata_w.var['med_nonz'].values \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff775d-3988-4580-b7b4-c348ba958f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "FILE = \"TS_epithelial.h5ad\"\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "(adata_w.X[mask, :] - adata.X[mask, :]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea8504-7206-427a-b4ad-5d0a58f8fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_w.X[mask, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef84ecb9-b34c-4db6-82e2-b5d99fa1a35e",
   "metadata": {},
   "source": [
    "### Scaling by median nonzero entry of each gene across all cells (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a102f-4053-420a-9d04-b21410d9853d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "# FILE = \"TS_epithelial.h5ad\"\n",
    "# adata_w = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "# #tf = 'DDX11L1'\n",
    "# tf_list =['DDX11L1', 'WASH7P', 'MIR6859-1']\n",
    "\n",
    "# def median_nonzero(col):\n",
    "#     nonzero_vals = col[col != 0] \n",
    "#     return np.median(nonzero_vals) if len(nonzero_vals) > 0 else 0\n",
    "\n",
    "# # requires scalar is a scalar\n",
    "# def perturb_counts(tf_list, scalar, adata): \n",
    "#     # compute nonzero median expression of each gene across cells, save to var\n",
    "#     med_nonz = np.apply_along_axis(median_nonzero, axis=0, arr=adata.X)\n",
    "#     adata.var['med_nonz'] = med_nonz\n",
    "\n",
    "#     # filter by desired tf(s), and apply the nonzero_median scaling operation to only these  \n",
    "#     mask = np.where(adata.var['gene_symbol'].isin(tf_list))[0]\n",
    "#     adata.X[mask, :] = adata.X[mask, :] * adata.var['med_nonz'].values * scalar\n",
    "#     return adata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde1fc75-0e49-4800-9c33-d147e4bac06d",
   "metadata": {},
   "source": [
    "### Scaling by max gene expression within each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ebbee6-3805-417a-9425-e99f2b8b35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version with (I believe) improper mask that was being applied to rows and not columns\n",
    "\n",
    "# import numpy as np\n",
    "# DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "# FILE = \"TS_epithelial.h5ad\"\n",
    "# adata_w = sp.read_h5ad(os.path.join(DATAPATH, FILE))\n",
    "# #tf = 'DDX11L1'\n",
    "# tf_list =['DDX11L1', 'WASH7P', 'MIR6859-1']\n",
    "\n",
    "\n",
    "# # requires scalar is a scalar\n",
    "# def perturb_counts(tf_list, scalar, adata): \n",
    "#     # compute nonzero median expression of each gene across cells, save to var\n",
    "#     med_nonz = med_nonz = np.max(adata.X, axis=1)\n",
    "#     adata.obs['med_nonz'] = med_nonz\n",
    "\n",
    "#     # filter by desired tf(s), and apply the nonzero_median scaling operation to only these  \n",
    "#     mask = np.where(adata.var['gene_symbol'].isin(tf_list))[0]\n",
    "#     adata.X[mask, :] = adata.X[mask, :] * adata.obs['med_nonz'].values * scalar\n",
    "#     return adata\n",
    "\n",
    "# old version without extra obs and var rows telling what was scaled and by how much\n",
    "\n",
    "\n",
    "# def perturb_counts(tf_list, scalar, adata): \n",
    "#     # Compute maximum expression level of each cell and save it to obs\n",
    "#     med_nonz = np.max(adata.X, axis=1)\n",
    "#     adata.obs['med_nonz'] = med_nonz\n",
    "    \n",
    "#     # apply operation only to genes in tf_list\n",
    "#     mask = np.where(adata.var['gene_symbol'].isin(tf_list))[0]\n",
    "#     adata.X[:, mask] = adata.X[:, mask] * adata.obs['med_nonz'].values[:, np.newaxis] * scalar\n",
    "    \n",
    "#     return adata\n",
    "\n",
    "# version before gpt optimized\n",
    "\n",
    "# def perturb_counts(tf_list, scalar, adata): \n",
    "#     # Compute maximum expression level of each cell and save it to obs\n",
    "#     med_nonz = np.max(adata.X, axis=1)\n",
    "#     adata.obs['med_nonz'] = med_nonz\n",
    "    \n",
    "#     # Add a new obs column called 'scalar' containing the scalar value for each row\n",
    "#     adata.obs['scalar'] = scalar\n",
    "    \n",
    "#     # Create a mask for genes in tf_list\n",
    "#     mask = np.where(adata.var['gene_symbol'].isin(tf_list))[0]\n",
    "#     # Apply the scaling operation to the specified genes\n",
    "#     adata.X[:, mask] = adata.X[:, mask] * adata.obs['med_nonz'].values[:, np.newaxis] * scalar\n",
    "    \n",
    "#     # Add a new var column called 'scaled' with True for genes in tf_list and False otherwise\n",
    "#     adata.var['scaled'] = adata.var['gene_symbol'].isin(tf_list)\n",
    "    \n",
    "#     # Add a new var column 'scaled_by'\n",
    "#     adata.var['scaled_by'] = 1\n",
    "#     # Set scaling factor for genes in tf_list\n",
    "\n",
    "#     ############I asked gpt to do this line and am unsure if it is correct. checking now.\n",
    "#     adata.var.loc[adata.var['scaled'], 'scaled_by'] = adata.obs['med_nonz'].values[:, np.newaxis] * scalar\n",
    "#     ###############\n",
    "    \n",
    "#     return adata\n",
    "\n",
    "# loop version before gpt optimized\n",
    "\n",
    "# # requires that within each perturbation, all of the transcription factors in tf_list are scaled by the same amount, that is, (scalar * \"max gene expression in that cell\")\n",
    "# # requires adata is cells x genes\n",
    "\n",
    "# import anndata\n",
    "\n",
    "# def iterate_perturb_counts(tf_list, scalar_list, adata):\n",
    "#     adata_dict = {}\n",
    "    \n",
    "#     for scalar in scalar_list:\n",
    "#         adata_temp = adata.copy()\n",
    "#         perturbed_adata = perturb_counts(tf_list, scalar, adata_temp)\n",
    "#         adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "#     return adata_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99114ac8-38af-488b-8856-acc26632b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Josh, please read: the adata.obs['scalar'] = scalar copies the scalar down for that call, associated with every cell in X. Same with ['scaled'] and ['scaled_by']\n",
    "in var. This is good in case the data is later appended into one anndata object.\n",
    "But my return from the perturb_counts loop (cell below this) is a dictionary of all of the perturb_counts, since appending along any axis will probably either overwrite\n",
    "obs or var.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def perturb_counts(tf_list, scalar, adata): \n",
    "    \"\"\"\n",
    "    Applies a perturbation to the expression data of specific genes in an AnnData object.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Computes the maximum gene expression level for each cell.\n",
    "    2. Applies a scaling operation to the expression levels of genes listed in `tf_list`.\n",
    "       - Each entry of these genes in the matrix is multiplied by the maximum expression level \n",
    "         of its respective cell and a specified scalar value.\n",
    "    3. Updates the AnnData object with new columns:\n",
    "       - 'scaled': A boolean column indicating whether each gene is in the `tf_list`.\n",
    "       - 'scaled_by': Contains the scaling factor used for each gene (the product of the maximum \n",
    "         expression level of each cell and the scalar), or `1` if the gene was not in `tf_list`.\n",
    "    \n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols to be perturbed.\n",
    "    scalar (float): The scalar value used to scale the expression levels.\n",
    "    adata (AnnData): The AnnData object containing gene expression data.\n",
    "\n",
    "    Returns:\n",
    "    AnnData: The updated AnnData object with applied perturbations and new columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute maximum expression level of each cell\n",
    "    med_nonz = np.max(adata.X, axis=1)\n",
    "    \n",
    "    # Create a boolean mask for genes in tf_list\n",
    "    gene_mask = adata.var['gene_symbol'].isin(tf_list)\n",
    "    \n",
    "    # Apply the scaling operation to the specified genes\n",
    "    adata.X[:, gene_mask] *= med_nonz[:, np.newaxis] * scalar\n",
    "    \n",
    "    # Add/Update 'scaled' column in var\n",
    "    adata.var['scaled'] = gene_mask\n",
    "    \n",
    "    # Add/Update 'scaled_by' column in var\n",
    "    adata.var['scaled_by'] = 1  # Default value for genes not in tf_list\n",
    "    adata.var.loc[gene_mask, 'scaled_by'] = med_nonz[:, np.newaxis] * scalar  # Correct scaling factor assignment\n",
    "    \n",
    "    return adata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a79c50-3dbb-4d6f-8723-e6f967f41893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata\n",
    "\n",
    "def iterate_perturb_counts(tf_list, scalar_list, adata):\n",
    "    \"\"\"\n",
    "    Applies perturbations to the expression data of specified transcription factors across multiple scalars \n",
    "    and stores the resulting AnnData objects in a dictionary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Iterates over a list of scalar values.\n",
    "    2. For each scalar, creates a copy of the AnnData object to preserve the original data.\n",
    "    3. Applies the `perturb_counts` function to scale the expression data of genes listed in `tf_list` by\n",
    "       the maximum gene expression of each cell and the current scalar.\n",
    "    4. Stores the perturbed AnnData object in a dictionary with the scalar as the key.\n",
    "\n",
    "    Parameters:\n",
    "    tf_list (list): A list of gene symbols (transcription factors) to be perturbed.\n",
    "    scalar_list (list): A list of scalar values for scaling the gene expression.\n",
    "    adata (AnnData): The AnnData object containing gene expression data (cells x genes).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are scalar values and values are the corresponding perturbed AnnData objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    adata_dict = {}\n",
    "    \n",
    "    for scalar in scalar_list:\n",
    "        # Create a copy of the AnnData object for each scalar value\n",
    "        adata_temp = adata.copy()\n",
    "        \n",
    "        # Apply perturb_counts to the copied AnnData object\n",
    "        perturbed_adata = perturb_counts(tf_list, scalar, adata_temp)\n",
    "        \n",
    "        # Store the perturbed AnnData object in the dictionary with scalar as the key\n",
    "        adata_dict[scalar] = perturbed_adata\n",
    "    \n",
    "    return adata_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f0a9c-18e0-4d3a-8efd-7993accb1048",
   "metadata": {},
   "source": [
    "## Visualize Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8542f13-3c5c-4130-9766-0d0a9ce55e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6474b05-224f-4677-902e-5b4962a4bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"/nfs/turbo/umms-indikar/shared/projects/DGC/data/tabula_sapiens/extract/\"\n",
    "FILE = \"TS_epithelial.h5ad\"\n",
    "\n",
    "adata = sp.read_h5ad(os.path.join(DATAPATH, FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a5de2-be21-489c-8184-b90f010b06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010c3f2-3294-4f18-bcad-8a9b3abf9b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f83ed-4955-4962-a629-f003027303e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X.max(axis=1) # what is the value of the highest expressed gene for each cell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210614f-0e87-4a8a-b1c5-1cc95c7380ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = 'DDX11L1'\n",
    "index = np.where(adata.var['gene_symbol'] == TF)[0]\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3d5ad-f2f0-446d-83f3-c63179d369c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var['gene_symbol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45904679-08f7-4ec7-b746-4017140120c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aa3850-58e1-41b4-9b2a-60db4bea28b1",
   "metadata": {},
   "source": [
    "## Build driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563c2da-5366-4738-a33b-2bfe1e881eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def main(job_number, parameter_file):\n",
    "    \"\"\"\n",
    "    This is the main function for the array job to perform the reprogramming experiment. job_number is a single parameter\n",
    "    that will be used to look up in a parameter table which model, reprogramming recipe, and other information relevant\n",
    "    to the test.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine embedding parameters and recipie\n",
    "    df_embedding_parameters = pd.read_csv(parameter_file)\n",
    "    TFs    = df_embedding_parameters['TFs'].values[job_number]\n",
    "    model  = df_embedding_parameters['model'].values[job_number]\n",
    "    source = df_embedding_parameters['source'].values[job_number]\n",
    "    target = df_embedding_parameters['target'].values[job_number]\n",
    "\n",
    "    # Load the source data\n",
    "    adata = \n",
    "\n",
    "    # Perturb the data\n",
    "    perturbed_adata = perturbation_model(adata, TFs)\n",
    "\n",
    "    # Generate embeddings\n",
    "    if model == 'geneformer':\n",
    "        adata_embedded = embed_geneformer([source_adata, perturbed_adata, target_adata])\n",
    "    elif model == 'tGPT':\n",
    "        adata_embedded = embed_tGPT([source_adata, perturbed_adata, target_adata])\n",
    "    elif model == 'scGTP':\n",
    "        adata_embedded = embed_scGTP([source_adata, perturbed_adata, target_adata])\n",
    "\n",
    "    # Save the results to a file\n",
    "\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f3c98-3474-4971-a559-f0a294dba782",
   "metadata": {},
   "source": [
    "## Build parameter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9f04aa-ab4c-44fc-86e0-f69d7f21451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_parameters = {\n",
    "    'source': [],\n",
    "    'target': [],\n",
    "    'TFs'   : [],\n",
    "    'model' : []\n",
    "}\n",
    "models = ['geneformer', 'tGPT', 'scGTP']\n",
    "\n",
    "df = pd.read_csv('data/first_5_recepies_8_29_2024.csv')\n",
    "\n",
    "for i in range(5):\n",
    "    TFs = df['TFs'].values[i].split()\n",
    "    source = df['Source'].values[i]\n",
    "    target = df['Target'].values[i]\n",
    "    for model in models:\n",
    "        embedding_parameters['TFs'].append(TFs)\n",
    "        embedding_parameters['source'].append(source)\n",
    "        embedding_parameters['target'].append(source)\n",
    "        embedding_parameters['model'].append(model)\n",
    "\n",
    "df_embedding_parameters = pd.DataFrame(embedding_parameters)\n",
    "\n",
    "df_embedding_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad22cdb-f1ac-48b6-8053-b69d91309c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e02020-7129-4182-9bef-6535490e26a9",
   "metadata": {},
   "source": [
    "# Day 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18574c-9e46-4c1d-9b77-b4f888ae0430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a5d48-99dc-4880-809d-519448cbfb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load known reprogramming regiems\n",
    "df = pd.read_csv('data/known-regiems-T1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457114f-57d5-45e9-ad8d-ecaa06a76d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of unique transcription factors\n",
    "TFs = []\n",
    "for regime in df['TFs'].unique():\n",
    "    TFs += regime.replace(',', '').split()\n",
    "TFs = list(set(TFs))\n",
    "print(f\"{len(TFs)=}\")\n",
    "TFs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
