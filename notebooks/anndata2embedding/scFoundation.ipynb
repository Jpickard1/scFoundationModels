{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adb4e440-832b-4fcd-899f-928fa5a0c915",
   "metadata": {},
   "source": [
    "# scFoundation: Preprocessing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6abf98-bf59-48f3-a16a-2c5771e48d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_readable_size(bytes, decimal_places=2):\n",
    "    \"\"\"Convert bytes to a human-readable format (e.g., MB, GB).\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if bytes < 1024:\n",
    "            return f\"{bytes:.{decimal_places}f} {unit}\"\n",
    "        bytes /= 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0cc1a39-0a6d-4822-a648-d3264f5d1c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "3\n",
      "0\n",
      "Tesla V100-PCIE-16GB\n",
      "None\n",
      "Allocated Memory: 0.00 B\n",
      "Reserved Memory: 0.00 B\n"
     ]
    }
   ],
   "source": [
    "# cuda appears particularlly finicky with this model. If cuda.synchrnize() doesn't run, then model won't run\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "print(torch.cuda.synchronize())\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print(f'Allocated Memory: {get_human_readable_size(torch.cuda.memory_allocated(device))}')\n",
    "print(f'Reserved Memory: {get_human_readable_size(torch.cuda.memory_reserved(device))}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc8bfb6f-28c9-4780-a277-d86b0a656741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import sys \n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "sys.path.append('/home/jpic/scFoundationProject/scFoundation/scFoundation/model')\n",
    "from pretrainmodels import select_model\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e447de7-d566-45a1-bb72-6932a03bb801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 0.00 B\n",
      "Reserved Memory: 0.00 B\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "print(f'Allocated Memory: {get_human_readable_size(torch.cuda.memory_allocated(device))}')\n",
    "print(f'Reserved Memory: {get_human_readable_size(torch.cuda.memory_reserved(device))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd0cff-6d71-4e38-9696-c97dcd5c630c",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccb60a33-e574-42ca-8086-95661d322c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_gene_selection(X_df, gene_list):\n",
    "    \"\"\"\n",
    "    Describe:\n",
    "        rebuild the input adata to select target genes encode protein \n",
    "    Parameters:\n",
    "        adata->`~anndata.AnnData` object: adata with var index_name by gene symbol\n",
    "        gene_list->list: wanted target gene \n",
    "    Returns:\n",
    "        adata_new->`~anndata.AnnData` object\n",
    "        to_fill_columns->list: zero padding gene\n",
    "    \"\"\"\n",
    "    to_fill_columns = list(set(gene_list) - set(X_df.columns))\n",
    "    padding_df = pd.DataFrame(np.zeros((X_df.shape[0], len(to_fill_columns))), \n",
    "                              columns=to_fill_columns, \n",
    "                              index=X_df.index)\n",
    "    X_df = pd.DataFrame(np.concatenate([df.values for df in [X_df, padding_df]], axis=1), \n",
    "                        index=X_df.index, \n",
    "                        columns=list(X_df.columns) + list(padding_df.columns))\n",
    "    X_df = X_df[gene_list]\n",
    "    \n",
    "    var = pd.DataFrame(index=X_df.columns)\n",
    "    var['mask'] = [1 if i in to_fill_columns else 0 for i in list(var.index)]\n",
    "    return X_df, to_fill_columns,var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f86d13-507f-4466-a8c0-caa47f10da3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '/nfs/turbo/umms-indikar/shared/projects/geneformer/data/rajapakse_lab_data_jpic.h5ad'\n",
    "adata         = sc.read_h5ad(input_file)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(adata.X.T, index=adata.var.index)\n",
    "gene_list_df = pd.read_csv('/nfs/turbo/umms-indikar/shared/projects/foundation_models/scFoundation/scFoundation/OS_scRNA_gene_index.19264.tsv', header=0, delimiter='\\t')\n",
    "gene_list = list(gene_list_df['gene_name'])\n",
    "X_df, to_fill_columns, var = main_gene_selection(df.T, gene_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13110b0-f093-455a-9aa8-0b6aef30888f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 0.00 B\n",
      "Reserved Memory: 0.00 B\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "print(f'Allocated Memory: {get_human_readable_size(torch.cuda.memory_allocated(device))}')\n",
    "print(f'Reserved Memory: {get_human_readable_size(torch.cuda.memory_reserved(device))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184ed9ca-4646-4b58-9573-fb567c73cc1f",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "544b0733-ed28-417e-b1d9-5f7b7f574fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)  # numpy random generator\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "\n",
    "def convertconfig(ckpt):\n",
    "    newconfig = {}\n",
    "    newconfig['config']={}\n",
    "    model_type = ckpt['config']['model']\n",
    "    \n",
    "    for key, val in ckpt['config']['model_config'][model_type].items():\n",
    "        newconfig['config'][key]=val\n",
    "        \n",
    "    for key, val in ckpt['config']['dataset_config']['rnaseq'].items():\n",
    "        newconfig['config'][key]=val\n",
    "        \n",
    "    if model_type == 'performergau_resolution':\n",
    "        model_type = 'performer_gau'\n",
    "    \n",
    "    import collections\n",
    "    d = collections.OrderedDict()\n",
    "    for key, val in ckpt['state_dict'].items():\n",
    "        d[str(key).split('model.')[1]]=val\n",
    "        \n",
    "    newconfig['config']['model_type']=model_type\n",
    "    newconfig['model_state_dict']=d\n",
    "    newconfig['config']['pos_embed']=False\n",
    "    newconfig['config']['device']='cuda'\n",
    "    return newconfig\n",
    "\n",
    "def gatherData(data, labels, pad_token_id):\n",
    "    \"\"\"\n",
    "    Gathers data and prepares it for model input by handling padding and sorting based on labels.\n",
    "\n",
    "    Parameters:\n",
    "    data (torch.Tensor): The input data tensor.\n",
    "    labels (torch.Tensor): The labels tensor indicating the presence of values in the data tensor.\n",
    "    pad_token_id (int): The token ID used for padding.\n",
    "\n",
    "    Returns:\n",
    "    new_data (torch.Tensor): The gathered data tensor with padding handled.\n",
    "    padding_labels (torch.Tensor): The tensor indicating which positions are padding.\n",
    "    \"\"\"\n",
    "    # Calculate the number of values per row\n",
    "    value_nums = labels.sum(1)\n",
    "    max_num = max(value_nums)\n",
    "\n",
    "    # Create fake data for padding\n",
    "    fake_data = torch.full((data.shape[0], max_num), pad_token_id, device=data.device)\n",
    "    data = torch.hstack([data, fake_data])\n",
    "\n",
    "    # Create fake labels for padding\n",
    "    fake_label = torch.full((labels.shape[0], max_num), 1, device=labels.device)\n",
    "    none_labels = ~labels\n",
    "    labels = labels.float()\n",
    "    labels[none_labels] = torch.tensor(-float('Inf'), device=labels.device)\n",
    "\n",
    "    # Create a tensor to adjust labels for sorting\n",
    "    tmp_data = torch.tensor([(i + 1) * 20000 for i in range(labels.shape[1], 0, -1)], device=labels.device)\n",
    "    labels += tmp_data\n",
    "\n",
    "    # Concatenate the original labels with fake labels\n",
    "    labels = torch.hstack([labels, fake_label])\n",
    "\n",
    "    # Sort and gather data based on the top-k labels\n",
    "    fake_label_gene_idx = labels.topk(max_num).indices\n",
    "    new_data = torch.gather(data, 1, fake_label_gene_idx)\n",
    "\n",
    "    # Determine which positions are padding\n",
    "    padding_labels = (new_data == pad_token_id)\n",
    "\n",
    "    return new_data, padding_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16bbad0c-89f8-4a47-8fe6-c1ac309118e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mask_gene_name': False, 'gene_num': 19266, 'seq_len': 19266, 'encoder': {'hidden_dim': 768, 'depth': 12, 'heads': 12, 'dim_head': 64, 'seq_len': 19266, 'module_type': 'transformer', 'norm_first': False}, 'decoder': {'hidden_dim': 512, 'depth': 6, 'heads': 8, 'dim_head': 64, 'module_type': 'performer', 'seq_len': 19266, 'norm_first': False}, 'n_class': 104, 'pad_token_id': 103, 'mask_token_id': 102, 'bin_num': 100, 'bin_alpha': 1.0, 'rawcount': True, 'model': 'mae_autobin', 'test_valid_train_idx_dict': '/nfs_beijing/minsheng/data/os10000w-new/global_shuffle/meta.csv.train_set_idx_dict.pt', 'valid_data_path': '/nfs_beijing/minsheng/data/valid_count_10w.npz', 'num_tokens': 13, 'train_data_path': None, 'isPanA': False, 'isPlanA1': False, 'max_files_to_load': 5, 'bin_type': 'auto_bin', 'value_mask_prob': 0.3, 'zero_mask_prob': 0.03, 'replace_prob': 0.8, 'random_token_prob': 0.1, 'mask_ignore_token_ids': [0], 'decoder_add_zero': True, 'mae_encoder_max_seq_len': 15000, 'isPlanA': False, 'mask_prob': 0.3, 'model_type': 'mae_autobin', 'pos_embed': False, 'device': 'cuda'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaeAutobin(\n",
       "  (token_emb): AutoDiscretizationEmbedding2(\n",
       "    (mlp): Linear(in_features=1, out_features=100, bias=True)\n",
       "    (mlp2): Linear(in_features=100, out_features=100, bias=True)\n",
       "    (LeakyReLU): LeakyReLU(negative_slope=0.1)\n",
       "    (Softmax): Softmax(dim=-1)\n",
       "    (emb): Embedding(100, 768)\n",
       "    (emb_mask): Embedding(1, 768)\n",
       "    (emb_pad): Embedding(1, 768)\n",
       "  )\n",
       "  (pos_emb): Embedding(19267, 768)\n",
       "  (decoder_embed): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (to_final): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (encoder): pytorchTransformerModule(\n",
       "    (transformer_encoder): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): PerformerModule(\n",
       "    (performer): Performer(\n",
       "      (net): SequentialSequence(\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x ModuleList(\n",
       "            (0): PreLayerNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): SelfAttention(\n",
       "                (fast_attention): FastAttention(\n",
       "                  (kernel_fn): ReLU()\n",
       "                )\n",
       "                (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (to_out): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): PreLayerNorm(\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (fn): Chunk(\n",
       "                (fn): FeedForward(\n",
       "                  (w1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                  (act): GELU(approximate='none')\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (w2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ckpt_path = '/nfs/turbo/umms-indikar/shared/projects/foundation_models/scFoundation/scFoundation/model/models/models.ckpt'\n",
    "key = 'cell'\n",
    "\n",
    "model_data = torch.load(best_ckpt_path,map_location='cpu')\n",
    "model_data = model_data[key]\n",
    "model_data = convertconfig(model_data)\n",
    "if not model_data.__contains__('config'):\n",
    "    print('***** No config *****')\n",
    "    config={}\n",
    "    config['model_type']='flash_all'\n",
    "else:\n",
    "    config=model_data['config']\n",
    "    print(config)\n",
    "if not config.__contains__('qv_dim'):\n",
    "    if config['model'] != 'mae_autobin':\n",
    "        if config.__contains__('dim_head'):\n",
    "            config['qv_dim']=config['dim_head']\n",
    "        else:\n",
    "            print('***** No qv_dim ***** set 64')\n",
    "            config['qv_dim']= 64\n",
    "if not config.__contains__('ppi_edge'):\n",
    "    config['ppi_edge']=None\n",
    "model = select_model(config)\n",
    "model_state_dict = model_data['model_state_dict']    \n",
    "model.load_state_dict(model_state_dict)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a41dc1f-0984-4fc6-abe3-b1865cc3e6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 455.36 MB\n",
      "Reserved Memory: 504.00 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "print(f'Allocated Memory: {get_human_readable_size(torch.cuda.memory_allocated(device))}')\n",
    "print(f'Reserved Memory: {get_human_readable_size(torch.cuda.memory_reserved(device))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eeb3bbc-a5db-4115-a628-751353517d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed(gexpr_feature, input_type='singlecell', pre_normalized='T', tgthighres='f0.5', output_type='cell', pool_type='all', \n",
    "          pretrainmodel=None, pretrainconfig=None, gatherData=None, getEncoerDecoderData=None, strname='output.npy'):\n",
    "    \"\"\"\n",
    "    Embeds gene expression data using a pre-trained model.\n",
    "\n",
    "    Parameters:\n",
    "    gexpr_feature (DataFrame): The gene expression feature data.\n",
    "    input_type (str): Type of input data ('bulk' or 'singlecell'). Default is 'singlecell'.\n",
    "    pre_normalized (str): Indicates if the data is pre-normalized ('T', 'F', 'A'). Default is 'T'.\n",
    "    tgthighres (str): Target high resolution ('f', 'a', 't' followed by a number). Default is 'f0.5'.\n",
    "    output_type (str): Type of output embedding ('cell', 'gene', 'gene_batch', 'gene_expression'). Default is 'cell'.\n",
    "    pool_type (str): Pooling type for embeddings ('all' or 'max'). Default is 'all'.\n",
    "    pretrainmodel (torch.nn.Module): The pre-trained model used for embedding.\n",
    "    pretrainconfig (dict): Configuration dictionary for the pre-trained model.\n",
    "    gatherData (function): Function to gather data for the model.\n",
    "    getEncoerDecoderData (function): Function to get encoder-decoder data.\n",
    "    strname (str): The name of the output file to save embeddings. Default is 'output.npy'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    geneexpemb = []\n",
    "    batchcontainer = []\n",
    "\n",
    "    for i in tqdm(range(gexpr_feature.shape[0])):\n",
    "        with torch.no_grad():\n",
    "            if input_type == 'bulk':\n",
    "                if pre_normalized == 'T':\n",
    "                    totalcount = gexpr_feature.iloc[i, :].sum()\n",
    "                elif pre_normalized == 'F':\n",
    "                    totalcount = np.log10(gexpr_feature.iloc[i, :].sum())\n",
    "                else:\n",
    "                    raise ValueError('pre_normalized must be T or F')\n",
    "                tmpdata         = gexpr_feature.iloc[i, :].tolist()\n",
    "                pretrain_gene_x = torch.tensor(tmpdata + [totalcount, totalcount]).unsqueeze(0).cuda()\n",
    "                data_gene_ids   = torch.arange(19266, device=pretrain_gene_x.device).repeat(pretrain_gene_x.shape[0], 1)\n",
    "            \n",
    "            elif input_type == 'singlecell':\n",
    "                if pre_normalized == 'F':\n",
    "                    tmpdata = np.log1p(gexpr_feature.iloc[i, :] / gexpr_feature.iloc[i, :].sum() * 1e4).tolist()\n",
    "                elif pre_normalized == 'T':\n",
    "                    tmpdata = gexpr_feature.iloc[i, :].tolist()\n",
    "                elif pre_normalized == 'A':\n",
    "                    tmpdata = gexpr_feature.iloc[i, :-1].tolist()\n",
    "                else:\n",
    "                    raise ValueError('pre_normalized must be T, F, or A')\n",
    "\n",
    "                if pre_normalized == 'A':\n",
    "                    totalcount = gexpr_feature.iloc[i, -1]\n",
    "                else:\n",
    "                    totalcount = gexpr_feature.iloc[i, :].sum()\n",
    "\n",
    "                if tgthighres[0] == 'f':\n",
    "                    pretrain_gene_x = torch.tensor(tmpdata + [np.log10(totalcount * float(tgthighres[1:])), np.log10(totalcount)]).unsqueeze(0).cuda()\n",
    "                elif tgthighres[0] == 'a':\n",
    "                    pretrain_gene_x = torch.tensor(tmpdata + [np.log10(totalcount) + float(tgthighres[1:]), np.log10(totalcount)]).unsqueeze(0).cuda()\n",
    "                elif tgthighres[0] == 't':\n",
    "                    pretrain_gene_x = torch.tensor(tmpdata + [float(tgthighres[1:]), np.log10(totalcount)]).unsqueeze(0).cuda()\n",
    "                else:\n",
    "                    raise ValueError('tgthighres must start with f, a, or t')\n",
    "                data_gene_ids = torch.arange(19266, device=pretrain_gene_x.device).repeat(pretrain_gene_x.shape[0], 1)\n",
    "\n",
    "            value_labels = pretrain_gene_x > 0\n",
    "            x, x_padding = gatherData(pretrain_gene_x, value_labels, pretrainconfig['pad_token_id'])\n",
    "\n",
    "            if output_type == 'cell':\n",
    "                position_gene_ids, _ = gatherData(data_gene_ids, value_labels, pretrainconfig['pad_token_id'])\n",
    "                x = pretrainmodel.token_emb(torch.unsqueeze(x, 2).float(), output_weight=0)\n",
    "                position_emb = pretrainmodel.pos_emb(position_gene_ids)\n",
    "                x += position_emb\n",
    "                geneemb = pretrainmodel.encoder(x, x_padding)\n",
    "\n",
    "                geneemb1 = geneemb[:, -1, :]\n",
    "                geneemb2 = geneemb[:, -2, :]\n",
    "                geneemb3, _ = torch.max(geneemb[:, :-2, :], dim=1)\n",
    "                geneemb4 = torch.mean(geneemb[:, :-2, :], dim=1)\n",
    "                if pool_type == 'all':\n",
    "                    geneembmerge = torch.concat([geneemb1, geneemb2, geneemb3, geneemb4], axis=1)\n",
    "                elif pool_type == 'max':\n",
    "                    geneembmerge, _ = torch.max(geneemb, dim=1)\n",
    "                else:\n",
    "                    raise ValueError('pool_type must be all or max')\n",
    "                geneexpemb.append(geneembmerge.detach().cpu().numpy())\n",
    "\n",
    "            elif output_type == 'gene':\n",
    "                pretrainmodel.to_final = None\n",
    "                encoder_data, encoder_position_gene_ids, encoder_data_padding, encoder_labels, decoder_data, decoder_data_padding, new_data_raw, data_mask_labels, decoder_position_gene_ids = getEncoerDecoderData(pretrain_gene_x.float(), pretrain_gene_x.float(), pretrainconfig)\n",
    "                out = pretrainmodel.forward(x=encoder_data, padding_label=encoder_data_padding,\n",
    "                                            encoder_position_gene_ids=encoder_position_gene_ids,\n",
    "                                            encoder_labels=encoder_labels,\n",
    "                                            decoder_data=decoder_data,\n",
    "                                            mask_gene_name=False,\n",
    "                                            mask_labels=None,\n",
    "                                            decoder_position_gene_ids=decoder_position_gene_ids,\n",
    "                                            decoder_data_padding_labels=decoder_data_padding)\n",
    "                out = out[:, :19264, :].contiguous()\n",
    "                geneexpemb.append(out.detach().cpu().numpy())\n",
    "\n",
    "            elif output_type == 'gene_batch':\n",
    "                batchcontainer.append(pretrain_gene_x.float())\n",
    "                if len(batchcontainer) == gexpr_feature.shape[0]:\n",
    "                    batchcontainer = torch.concat(batchcontainer, axis=0)\n",
    "                else:\n",
    "                    continue\n",
    "                pretrainmodel.to_final = None\n",
    "                encoder_data, encoder_position_gene_ids, encoder_data_padding, encoder_labels, decoder_data, decoder_data_padding, new_data_raw, data_mask_labels, decoder_position_gene_ids = getEncoerDecoderData(batchcontainer, batchcontainer, pretrainconfig)\n",
    "                out = pretrainmodel.forward(x=encoder_data, padding_label=encoder_data_padding,\n",
    "                                            encoder_position_gene_ids=encoder_position_gene_ids,\n",
    "                                            encoder_labels=encoder_labels,\n",
    "                                            decoder_data=decoder_data,\n",
    "                                            mask_gene_name=False,\n",
    "                                            mask_labels=None,\n",
    "                                            decoder_position_gene_ids=decoder_position_gene_ids,\n",
    "                                            decoder_data_padding_labels=decoder_data_padding)\n",
    "                geneexpemb = out[:, :19264, :].contiguous().detach().cpu().numpy()\n",
    "\n",
    "            elif output_type == 'gene_expression':\n",
    "                encoder_data, encoder_position_gene_ids, encoder_data_padding, encoder_labels, decoder_data, decoder_data_padding, new_data_raw, data_mask_labels, decoder_position_gene_ids = getEncoerDecoderData(pretrain_gene_x.float(), pretrain_gene_x.float(), pretrainconfig)\n",
    "                out = pretrainmodel.forward(x=encoder_data, padding_label=encoder_data_padding,\n",
    "                                            encoder_position_gene_ids=encoder_position_gene_ids,\n",
    "                                            encoder_labels=encoder_labels,\n",
    "                                            decoder_data=decoder_data,\n",
    "                                            mask_gene_name=False,\n",
    "                                            mask_labels=None,\n",
    "                                            decoder_position_gene_ids=decoder_position_gene_ids,\n",
    "                                            decoder_data_padding_labels=decoder_data_padding)\n",
    "                out = out[:, :19264].contiguous()\n",
    "                geneexpemb.append(out.detach().cpu().numpy())                \n",
    "            else:\n",
    "                raise ValueError('output_type must be cell, gene, gene_batch, or gene_expression')\n",
    "\n",
    "    geneexpemb = np.squeeze(np.array(geneexpemb))\n",
    "    return geneexpemb\n",
    "    # print(geneexpemb.shape)\n",
    "    # np.save(strname, geneexpemb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43b9dc44-df22-4def-ade6-ca9613d404fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 455.36 MB\n",
      "Reserved Memory: 504.00 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "print(f'Allocated Memory: {get_human_readable_size(torch.cuda.memory_allocated(device))}')\n",
    "print(f'Reserved Memory: {get_human_readable_size(torch.cuda.memory_reserved(device))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51776f3e-55bf-4ea3-a3bd-75279cbf79f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1acecc2-334b-4582-b237-9b65ab4c6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.reset_peak_memory_stats()\n",
    "# torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5122dbf3-e040-4858-bd0f-0a940ef9bdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 455.36 MB\n",
      "Reserved Memory: 504.00 MB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "print(f'Allocated Memory: {get_human_readable_size(torch.cuda.memory_allocated(device))}')\n",
    "print(f'Reserved Memory: {get_human_readable_size(torch.cuda.memory_reserved(device))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13e1097b-45ea-4d51-bfd7-ea83156be34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/66 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 8.58 GiB. GPU 0 has a total capacty of 15.77 GiB of which 6.10 GiB is free. Including non-PyTorch memory, this process has 9.66 GiB memory in use. Of the allocated memory 707.76 MiB is allocated by PyTorch, and 8.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the cell below before this one\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msinglecell\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgthighres\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf0.5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcell\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m          \u001b[49m\u001b[43mpretrainmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrainconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgatherData\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgatherData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgetEncoerDecoderData\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 75\u001b[0m, in \u001b[0;36membed\u001b[0;34m(gexpr_feature, input_type, pre_normalized, tgthighres, output_type, pool_type, pretrainmodel, pretrainconfig, gatherData, getEncoerDecoderData, strname)\u001b[0m\n\u001b[1;32m     73\u001b[0m position_emb \u001b[38;5;241m=\u001b[39m pretrainmodel\u001b[38;5;241m.\u001b[39mpos_emb(position_gene_ids)\n\u001b[1;32m     74\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_emb\n\u001b[0;32m---> 75\u001b[0m geneemb \u001b[38;5;241m=\u001b[39m \u001b[43mpretrainmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_padding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m geneemb1 \u001b[38;5;241m=\u001b[39m geneemb[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     78\u001b[0m geneemb2 \u001b[38;5;241m=\u001b[39m geneemb[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, :]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scFoundationProject/scFoundation/scFoundation/model/pretrainmodels/transformer.py:39\u001b[0m, in \u001b[0;36mpytorchTransformerModule.forward\u001b[0;34m(self, x, padding_mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# x get encodings [B, N, D] , batch_first is True\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder:\n\u001b[0;32m---> 39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# , src_mask=mask, src_key_padding_mask=src_key_padding_mask)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# x = self.transformer_encoder(x)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:707\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    705\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 707\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    708\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:715\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    714\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 715\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/functional.py:5440\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5437\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   5438\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 5440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5441\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5443\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 8.58 GiB. GPU 0 has a total capacty of 15.77 GiB of which 6.10 GiB is free. Including non-PyTorch memory, this process has 9.66 GiB memory in use. Of the allocated memory 707.76 MiB is allocated by PyTorch, and 8.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Run the cell below before this one\n",
    "embeddings = embed(X_df, input_type='singlecell', pre_normalized='T', tgthighres='f0.5', output_type='cell', pool_type='all', \n",
    "          pretrainmodel=model, pretrainconfig=config, gatherData=gatherData, getEncoerDecoderData=None, strname='output.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf19466a-743c-4a44-8136-16d33fbf977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated Memory: 707.76 MB\n",
      "Reserved Memory: 9.30 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "print(f'Allocated Memory: {get_human_readable_size(torch.cuda.memory_allocated(device))}')\n",
    "print(f'Reserved Memory: {get_human_readable_size(torch.cuda.memory_reserved(device))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "031c485c-ff54-4061-bb39-85dc80dd9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def embed(gexpr_feature, input_type='singlecell', pre_normalized='T', tgthighres='f0.5', output_type='cell', pool_type='all', \n",
    "          pretrainmodel=None, pretrainconfig=None, gatherData=None, getEncoerDecoderData=None, strname='output.npy'):\n",
    "    \"\"\"\n",
    "    Embeds gene expression data using a pre-trained model.\n",
    "\n",
    "    Parameters:\n",
    "    gexpr_feature (DataFrame): The gene expression feature data.\n",
    "    input_type (str): Type of input data ('bulk' or 'singlecell'). Default is 'singlecell'.\n",
    "    pre_normalized (str): Indicates if the data is pre-normalized ('T', 'F', 'A'). Default is 'T'.\n",
    "    tgthighres (str): Target high resolution ('f', 'a', 't' followed by a number). Default is 'f0.5'.\n",
    "    output_type (str): Type of output embedding ('cell', 'gene', 'gene_batch', 'gene_expression'). Default is 'cell'.\n",
    "    pool_type (str): Pooling type for embeddings ('all' or 'max'). Default is 'all'.\n",
    "    pretrainmodel (torch.nn.Module): The pre-trained model used for embedding.\n",
    "    pretrainconfig (dict): Configuration dictionary for the pre-trained model.\n",
    "    gatherData (function): Function to gather data for the model.\n",
    "    getEncoerDecoderData (function): Function to get encoder-decoder data.\n",
    "    strname (str): The name of the output file to save embeddings. Default is 'output.npy'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    geneexpemb = []\n",
    "    batchcontainer = []\n",
    "\n",
    "    for i in tqdm(range(gexpr_feature.shape[0])):\n",
    "        with torch.no_grad():\n",
    "            if input_type == 'bulk':\n",
    "                if pre_normalized == 'T':\n",
    "                    totalcount = gexpr_feature.iloc[i, :].sum()\n",
    "                elif pre_normalized == 'F':\n",
    "                    totalcount = np.log10(gexpr_feature.iloc[i, :].sum())\n",
    "                else:\n",
    "                    raise ValueError('pre_normalized must be T or F')\n",
    "                tmpdata         = gexpr_feature.iloc[i, :].tolist()\n",
    "                pretrain_gene_x = torch.tensor(tmpdata + [totalcount, totalcount]).unsqueeze(0).cuda()\n",
    "                data_gene_ids   = torch.arange(19266, device=pretrain_gene_x.device).repeat(pretrain_gene_x.shape[0], 1)\n",
    "            \n",
    "            elif input_type == 'singlecell':\n",
    "                if pre_normalized == 'F':\n",
    "                    tmpdata = np.log1p(gexpr_feature.iloc[i, :] / gexpr_feature.iloc[i, :].sum() * 1e4).tolist()\n",
    "                elif pre_normalized == 'T':\n",
    "                    tmpdata = gexpr_feature.iloc[i, :].tolist()\n",
    "                elif pre_normalized == 'A':\n",
    "                    tmpdata = gexpr_feature.iloc[i, :-1].tolist()\n",
    "                else:\n",
    "                    raise ValueError('pre_normalized must be T, F, or A')\n",
    "\n",
    "                if pre_normalized == 'A':\n",
    "                    totalcount = gexpr_feature.iloc[i, -1]\n",
    "                else:\n",
    "                    totalcount = gexpr_feature.iloc[i, :].sum()\n",
    "\n",
    "                if tgthighres[0] == 'f':\n",
    "                    pretrain_gene_x = torch.tensor(tmpdata + [np.log10(totalcount * float(tgthighres[1:])), np.log10(totalcount)]).unsqueeze(0).cuda()\n",
    "                elif tgthighres[0] == 'a':\n",
    "                    pretrain_gene_x = torch.tensor(tmpdata + [np.log10(totalcount) + float(tgthighres[1:]), np.log10(totalcount)]).unsqueeze(0).cuda()\n",
    "                elif tgthighres[0] == 't':\n",
    "                    pretrain_gene_x = torch.tensor(tmpdata + [float(tgthighres[1:]), np.log10(totalcount)]).unsqueeze(0).cuda()\n",
    "                else:\n",
    "                    raise ValueError('tgthighres must start with f, a, or t')\n",
    "                data_gene_ids = torch.arange(19266, device=pretrain_gene_x.device).repeat(pretrain_gene_x.shape[0], 1)\n",
    "\n",
    "            value_labels = pretrain_gene_x > 0\n",
    "            x, x_padding = gatherData(pretrain_gene_x, value_labels, pretrainconfig['pad_token_id'])\n",
    "\n",
    "            if output_type == 'cell':\n",
    "                position_gene_ids, _ = gatherData(data_gene_ids, value_labels, pretrainconfig['pad_token_id'])\n",
    "                x = pretrainmodel.token_emb(torch.unsqueeze(x, 2).float(), output_weight=0)\n",
    "                position_emb = pretrainmodel.pos_emb(position_gene_ids)\n",
    "                x += position_emb\n",
    "                geneemb = pretrainmodel.encoder(x, x_padding)\n",
    "\n",
    "                geneemb1 = geneemb[:, -1, :]\n",
    "                geneemb2 = geneemb[:, -2, :]\n",
    "                geneemb3, _ = torch.max(geneemb[:, :-2, :], dim=1)\n",
    "                geneemb4 = torch.mean(geneemb[:, :-2, :], dim=1)\n",
    "                if pool_type == 'all':\n",
    "                    geneembmerge = torch.concat([geneemb1, geneemb2, geneemb3, geneemb4], axis=1)\n",
    "                elif pool_type == 'max':\n",
    "                    geneembmerge, _ = torch.max(geneemb, dim=1)\n",
    "                else:\n",
    "                    raise ValueError('pool_type must be all or max')\n",
    "                geneexpemb.append(geneembmerge.detach().cpu().numpy())\n",
    "\n",
    "            elif output_type == 'gene':\n",
    "                pretrainmodel.to_final = None\n",
    "                encoder_data, encoder_position_gene_ids, encoder_data_padding, encoder_labels, decoder_data, decoder_data_padding, new_data_raw, data_mask_labels, decoder_position_gene_ids = getEncoerDecoderData(pretrain_gene_x.float(), pretrain_gene_x.float(), pretrainconfig)\n",
    "                out = pretrainmodel.forward(x=encoder_data, padding_label=encoder_data_padding,\n",
    "                                            encoder_position_gene_ids=encoder_position_gene_ids,\n",
    "                                            encoder_labels=encoder_labels,\n",
    "                                            decoder_data=decoder_data,\n",
    "                                            mask_gene_name=False,\n",
    "                                            mask_labels=None,\n",
    "                                            decoder_position_gene_ids=decoder_position_gene_ids,\n",
    "                                            decoder_data_padding_labels=decoder_data_padding)\n",
    "                out = out[:, :19264, :].contiguous()\n",
    "                geneexpemb.append(out.detach().cpu().numpy())\n",
    "\n",
    "            elif output_type == 'gene_batch':\n",
    "                batchcontainer.append(pretrain_gene_x.float())\n",
    "                if len(batchcontainer) == gexpr_feature.shape[0]:\n",
    "                    batchcontainer = torch.concat(batchcontainer, axis=0)\n",
    "                else:\n",
    "                    continue\n",
    "                pretrainmodel.to_final = None\n",
    "                encoder_data, encoder_position_gene_ids, encoder_data_padding, encoder_labels, decoder_data, decoder_data_padding, new_data_raw, data_mask_labels, decoder_position_gene_ids = getEncoerDecoderData(batchcontainer, batchcontainer, pretrainconfig)\n",
    "                out = pretrainmodel.forward(x=encoder_data, padding_label=encoder_data_padding,\n",
    "                                            encoder_position_gene_ids=encoder_position_gene_ids,\n",
    "                                            encoder_labels=encoder_labels,\n",
    "                                            decoder_data=decoder_data,\n",
    "                                            mask_gene_name=False,\n",
    "                                            mask_labels=None,\n",
    "                                            decoder_position_gene_ids=decoder_position_gene_ids,\n",
    "                                            decoder_data_padding_labels=decoder_data_padding)\n",
    "                geneexpemb = out[:, :19264, :].contiguous().detach().cpu().numpy()\n",
    "\n",
    "            elif output_type == 'gene_expression':\n",
    "                encoder_data, encoder_position_gene_ids, encoder_data_padding, encoder_labels, decoder_data, decoder_data_padding, new_data_raw, data_mask_labels, decoder_position_gene_ids = getEncoerDecoderData(pretrain_gene_x.float(), pretrain_gene_x.float(), pretrainconfig)\n",
    "                out = pretrainmodel.forward(x=encoder_data, padding_label=encoder_data_padding,\n",
    "                                            encoder_position_gene_ids=encoder_position_gene_ids,\n",
    "                                            encoder_labels=encoder_labels,\n",
    "                                            decoder_data=decoder_data,\n",
    "                                            mask_gene_name=False,\n",
    "                                            mask_labels=None,\n",
    "                                            decoder_position_gene_ids=decoder_position_gene_ids,\n",
    "                                            decoder_data_padding_labels=decoder_data_padding)\n",
    "                out = out[:, :19264].contiguous()\n",
    "                geneexpemb.append(out.detach().cpu().numpy())                \n",
    "            else:\n",
    "                raise ValueError('output_type must be cell, gene, gene_batch, or gene_expression')\n",
    "\n",
    "    geneexpemb = np.squeeze(np.array(geneexpemb))\n",
    "    return geneexpemb\n",
    "    # print(geneexpemb.shape)\n",
    "    # np.save(strname, geneexpemb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd4bca-afc9-425f-a0d2-06a5dcf5e895",
   "metadata": {},
   "source": [
    "## Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b1c846b-112c-4951-bbc5-05edf6b9fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)  # numpy random generator\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(device)\n",
    "\n",
    "def convertconfig(ckpt):\n",
    "    newconfig = {}\n",
    "    newconfig['config']={}\n",
    "    model_type = ckpt['config']['model']\n",
    "    \n",
    "    for key, val in ckpt['config']['model_config'][model_type].items():\n",
    "        newconfig['config'][key]=val\n",
    "        \n",
    "    for key, val in ckpt['config']['dataset_config']['rnaseq'].items():\n",
    "        newconfig['config'][key]=val\n",
    "        \n",
    "    if model_type == 'performergau_resolution':\n",
    "        model_type = 'performer_gau'\n",
    "    \n",
    "    import collections\n",
    "    d = collections.OrderedDict()\n",
    "    for key, val in ckpt['state_dict'].items():\n",
    "        d[str(key).split('model.')[1]]=val\n",
    "        \n",
    "    newconfig['config']['model_type']=model_type\n",
    "    newconfig['model_state_dict']=d\n",
    "    newconfig['config']['pos_embed']=False\n",
    "    newconfig['config']['device']='cuda'\n",
    "    return newconfig\n",
    "\n",
    "def loaddata(data_path, verbose=True, pre_normalized='T', input_type='singlecell', demo=False):\n",
    "    #Load data\n",
    "    if data_path[-3:]=='npz':\n",
    "        gexpr_feature = scipy.sparse.load_npz(data_path)\n",
    "        gexpr_feature = pd.DataFrame(gexpr_feature.toarray())\n",
    "    elif data_path[-4:]=='h5ad':\n",
    "        gexpr_feature = sc.read_h5ad(data_path)\n",
    "        idx = gexpr_feature.obs_names.tolist()\n",
    "        col = gexpr_feature.var.gene_name.tolist()\n",
    "        if issparse(gexpr_feature.X):\n",
    "            gexpr_feature = gexpr_feature.X.toarray()\n",
    "        else:\n",
    "            gexpr_feature = gexpr_feature\n",
    "        gexpr_feature = pd.DataFrame(gexpr_feature,index=idx,columns=col)\n",
    "    elif data_path[-3:]=='npy':\n",
    "        gexpr_feature = np.load(data_path)\n",
    "        gexpr_feature = pd.DataFrame(gexpr_feature)\n",
    "    else:\n",
    "        gexpr_feature=pd.read_csv(data_path,index_col=0)\n",
    "    \n",
    "    if gexpr_feature.shape[1]<19264:\n",
    "        print('covert gene feature into 19264')\n",
    "        gexpr_feature, to_fill_columns,var = main_gene_selection(gexpr_feature,gene_list)\n",
    "        assert gexpr_feature.shape[1]>=19264\n",
    "    \n",
    "    if (pre_normalized == 'F') and (input_type == 'bulk'):\n",
    "        adata = sc.AnnData(gexpr_feature)\n",
    "        sc.pp.normalize_total(adata)\n",
    "        sc.pp.log1p(adata)\n",
    "        gexpr_feature = pd.DataFrame(adata.X,index=adata.obs_names,columns=adata.var_names)\n",
    "\n",
    "    if demo:\n",
    "        gexpr_feature = gexpr_feature.iloc[:10,:]\n",
    "    if verbose:\n",
    "        print(f'data.shape={gexpr_feature.shape}')\n",
    "    return gexpr_feature\n",
    "\n",
    "def gatherData(data, labels, pad_token_id):\n",
    "    \"\"\"\n",
    "    Gathers data and prepares it for model input by handling padding and sorting based on labels.\n",
    "\n",
    "    Parameters:\n",
    "    data (torch.Tensor): The input data tensor.\n",
    "    labels (torch.Tensor): The labels tensor indicating the presence of values in the data tensor.\n",
    "    pad_token_id (int): The token ID used for padding.\n",
    "\n",
    "    Returns:\n",
    "    new_data (torch.Tensor): The gathered data tensor with padding handled.\n",
    "    padding_labels (torch.Tensor): The tensor indicating which positions are padding.\n",
    "    \"\"\"\n",
    "    # Calculate the number of values per row\n",
    "    value_nums = labels.sum(1)\n",
    "    max_num = max(value_nums)\n",
    "\n",
    "    # Create fake data for padding\n",
    "    fake_data = torch.full((data.shape[0], max_num), pad_token_id, device=data.device)\n",
    "    data = torch.hstack([data, fake_data])\n",
    "\n",
    "    # Create fake labels for padding\n",
    "    fake_label = torch.full((labels.shape[0], max_num), 1, device=labels.device)\n",
    "    none_labels = ~labels\n",
    "    labels = labels.float()\n",
    "    labels[none_labels] = torch.tensor(-float('Inf'), device=labels.device)\n",
    "\n",
    "    # Create a tensor to adjust labels for sorting\n",
    "    tmp_data = torch.tensor([(i + 1) * 20000 for i in range(labels.shape[1], 0, -1)], device=labels.device)\n",
    "    labels += tmp_data\n",
    "\n",
    "    # Concatenate the original labels with fake labels\n",
    "    labels = torch.hstack([labels, fake_label])\n",
    "\n",
    "    # Sort and gather data based on the top-k labels\n",
    "    fake_label_gene_idx = labels.topk(max_num).indices\n",
    "    new_data = torch.gather(data, 1, fake_label_gene_idx)\n",
    "\n",
    "    # Determine which positions are padding\n",
    "    padding_labels = (new_data == pad_token_id)\n",
    "\n",
    "    return new_data, padding_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01544740-19b9-44af-8a17-361cb3d4f420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape=(10, 19264)\n"
     ]
    }
   ],
   "source": [
    "DATAPATH = '/nfs/turbo/umms-indikar/shared/projects/foundation_models/example_inputs/scFoundation/cell_type_rawdata/zheng/data_test_count.npy'\n",
    "data = loaddata(DATAPATH, demo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a5454d-1ee5-45c4-b8b2-533562c1d9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 2., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gexpr_feature = np.load(DATAPATH)\n",
    "gexpr_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15347e70-2fac-4c33-bebe-30a2e05a41c8",
   "metadata": {},
   "source": [
    "## Set Data & Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4486b78-a1a3-48c6-8cbe-b87f8d06dd52",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scFoundation",
   "language": "python",
   "name": "scfoundation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
