{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9466ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-06T03:28:37.515446Z",
     "iopub.status.busy": "2023-02-06T03:28:37.514988Z",
     "iopub.status.idle": "2023-02-06T03:28:46.605988Z",
     "shell.execute_reply": "2023-02-06T03:28:46.604772Z",
     "shell.execute_reply.started": "2023-02-06T03:28:37.515308Z"
    }
   },
   "source": [
    "# Minimal Working Example to run tGPT on Great Lakes\n",
    "\n",
    "The model parameters are stored at `/nfs/turbo/umms-indikar/shared/projects/foundation_models/transcriptome-gpt-1024-8-16-64` and some example data is located at `/nfs/turbo/umms-indikar/shared/projects/foundation_models/example_inputs/tGPT`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df4a8f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9b81f9",
   "metadata": {},
   "source": [
    "# Setting parameter and file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42839744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "print(device)\n",
    "tokenizer_file = \"/nfs/turbo/umms-indikar/shared/projects/foundation_models/transcriptome-gpt-1024-8-16-64\" \n",
    "checkpoint     = \"/nfs/turbo/umms-indikar/shared/projects/foundation_models/transcriptome-gpt-1024-8-16-64\" ## Pretrained model\n",
    "celltype_path  = \"/nfs/turbo/umms-indikar/shared/projects/foundation_models/example_inputs/tGPT/Muris_cell_labels.txt.gz\" ## Cell type annotation\n",
    "max_len        = 64 ## Number of top genes used for analysis\n",
    "text_file      = \"/nfs/turbo/umms-indikar/shared/projects/foundation_models/example_inputs/tGPT/Muris_gene_rankings.txt.gz\"  ## Gene symbols ranked by exprssion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8aa725",
   "metadata": {},
   "source": [
    "# Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ad57c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpic/.local/lib/python3.11/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model set\n",
      "model to device\n",
      "model eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 858/858 [01:59<00:00,  7.19it/s]\n"
     ]
    }
   ],
   "source": [
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines):\n",
    "        self.lines = lines\n",
    "        self.regex = re.compile(r'\\-|\\.')\n",
    "    def __getitem__(self, i):\n",
    "        return self.regex.sub('_', self.lines[i])\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_file)\n",
    "print('Tokenizer set')\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint,output_hidden_states = True).transformer\n",
    "print('model set')\n",
    "model = model.to(device)\n",
    "print('model to device')\n",
    "model.eval()\n",
    "print('model eval')\n",
    "\n",
    "lines = [s.decode().strip() for s in gzip.open(text_file, \"r\").readlines()]\n",
    "\n",
    "ds = LineDataset(lines)\n",
    "dl = DataLoader(ds, batch_size=64)\n",
    "\n",
    "Xs = []\n",
    "for a in tqdm(dl, total=len(dl)):\n",
    "    batch = tokenizer(a, max_length= max_len, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    for k, v in batch.items():\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x = model(**batch)\n",
    "    \n",
    "    eos_idxs = batch.attention_mask.sum(dim=1) - 1\n",
    "    xx = x.last_hidden_state\n",
    "       \n",
    "    result_list = [[] for i in range(len(xx))]\n",
    "\n",
    "    for j, item in enumerate(xx):\n",
    "        result_list[j] = item[1:int(eos_idxs[j]),:].mean(dim =0).tolist()\n",
    "        \n",
    "    Xs.extend(result_list)\n",
    "    \n",
    "features = np.stack(Xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56520b",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de78d116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata=sc.AnnData(features)\n",
    "celltype = pd.read_csv(celltype_path, header=None)[0].tolist()\n",
    "adata.obs[\"celltype\"] = celltype\n",
    "adata.obs[\"celltype\"] = adata.obs[\"celltype\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48c4d0eb-ab52-45fb-97fc-c335be7f319d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You’re trying to run this on 1024 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
      "         Falling back to preprocessing with `sc.pp.pca` and default params.\n"
     ]
    }
   ],
   "source": [
    "sc.pp.neighbors(adata,n_neighbors=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea76cf36-c617-49c9-acda-d0fc4684bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3663416/2288680387.py:1: FutureWarning: In the future, the default backend for leiden will be igraph instead of leidenalg.\n",
      "\n",
      " To achieve the future defaults please pass: flavor=\"igraph\" and n_iterations=2.  directed must also be False to work with igraph's implementation.\n",
      "  sc.tl.leiden(adata,resolution=0.6)\n"
     ]
    }
   ],
   "source": [
    "sc.tl.leiden(adata,resolution=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f6465e-85ec-4bb5-b025-23d478dcf8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.umap(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b4abf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################  Cell Type  #######################\n",
    "sc.pl.umap(adata, color = [\"celltype\"], show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d34ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############ Single-cell Clustering  #############\n",
    "sc.pl.umap(adata, color = [\"leiden\"], show = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7806d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(dpi=300,figsize=(7,7))\n",
    "sc.pl.umap(adata, color = [\"celltype\"], save=\"celltype.png\")\n",
    "sc.pl.umap(adata, color = [\"leiden\"], save=\"leiden.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d942ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61afa13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
